{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import ftplib\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime \n",
    "import time\n",
    "from datetime import datetime \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import scipy.stats as stats\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "##import sklearn\n",
    "#from sklearn.metrics import median_absolute_error, mean_squared_error,r2_score\n",
    "#from sklearn.linear_model import LinearRegression, RANSACRegressor, HuberRegressor\n",
    "\n",
    "\n",
    "import pyOptimalEstimation as pyOE\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cmocean import cm as cmo\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from glob import glob\n",
    "\n",
    "#import imageio\n",
    "\n",
    "#from pathlib import Path\n",
    "\n",
    "#from netCDF4 import Dataset\n",
    "\n",
    "import gc, psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/nobackup/users/echeverr/Git/SatOpEst/support') # where supporting_routines_m live\n",
    "#sys.path.append('/home/mario/Documents/work/code/git/SatOpEst/support') # where supporting_routines_m live\n",
    "\n",
    "import supporting_routines_m \n",
    "\n",
    "import os\n",
    "\n",
    "rttov_installdir = '/usr/people/echeverr/Documents/code/nwpsaf/rttov13'\n",
    "#rttov_installdir = '/home/mario/myLibs/rrtov13/rttov130'\n",
    "\n",
    "sys.path.append(rttov_installdir+'/wrapper')\n",
    "import pyrttov\n",
    "\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "#os.environ[\"CARTOPY_USER_BACKGROUNDS\"] = os.path.join(current_directory,'/nobackup/users/echeverr/py_tests/earthpy_example/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = psutil.Process()\n",
    "\n",
    "BT_dir = '/nobackup/users/echeverr/data/cmsaf/ssmis/F16/'\n",
    "#BT_dir = '/home/mario/Data/CMSAF/ssims/F16/test_1ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BT_file = '*.nc'\n",
    "#BT_file = 'BTRin20140909000000324SSF1601GL' #BTRin20140910000000324SSF1601GL\n",
    "#BT_file = 'BTRin20140910000000324SSF1601GL'\n",
    "BT_file = 'BTRin20140911000000324SSF1601GL'\n",
    "\n",
    "BT_file2 = BT_file+'_v1'\n",
    "\n",
    "\n",
    "# Chunking the dataset (for 30 mins 10,10 works well (in my laptop, Mario); \n",
    "# if increase minutes times x, then increase chunk_size_time times x as well (avoids memory problems))\n",
    "chunk_size_time =  10 # 420 for half day\n",
    "chunk_size_s_a_t = 10\n",
    "\n",
    "# user input:\n",
    "#init_date = np.datetime64('2014-09-09T00:25:00.000') \n",
    "#end_date = np.datetime64('2014-09-09T00:30:00.000')\n",
    "init_date = np.datetime64('2014-09-11T00:00:00.000') \n",
    "end_date = np.datetime64('2014-09-11T23:59:59.000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "import pyresample\n",
    "from pyresample import create_area_def, load_area, data_reduce, utils, AreaDefinition\n",
    "from pyresample.geometry import SwathDefinition\n",
    "from pyresample.kd_tree import resample_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Netcdf using xarray:\n",
    "def read_netcdfs(files, dim, transform_func=None, groups = None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "        with xr.open_dataset(path, group = groups) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "            ds.load()\n",
    "            return ds\n",
    "\n",
    "    paths = sorted(glob(files))\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined\n",
    "\n",
    "# Apply usual flags to CMSAF dataset (per scene)\n",
    "# Author: M. Echeverri, March 2021.\n",
    "# TODO:\n",
    "# - Add list of flags that the user wants to apply\n",
    "# - Add input checks\n",
    "\n",
    "def apply_scene_flags(scene_BT, BT_attributes):\n",
    "    \n",
    "    for i, scene in enumerate(scene_BT):\n",
    "        #scene_aux = scene\n",
    "        scene_aux = xr.where((scene.qc_fov==0), \n",
    "                                      scene, \n",
    "                                      np.nan)    # Apply 'qc_fov' flag \n",
    "        \n",
    "        scene_aux[\"tb\"] = xr.where((scene[\"tb\"]!=np.nan), \n",
    "                                      (scene[\"tb\"] + scene[\"ical\"]), \n",
    "                                          np.nan)  # Apply intercalibration offsets         \n",
    "\n",
    "        j = 0\n",
    "        for ch in scene.scene_channel.values:              # Apply 'qc_channel' flag \n",
    "            pos = (BT_attributes.qc_channel[:,scene.scene_channel[j]].values!=0)\n",
    "            scene_aux[\"tb\"][pos,j,:] = np.nan\n",
    "            j+=1\n",
    "\n",
    "        scene_BT[i] = scene_aux    \n",
    "    return scene_BT\n",
    "\n",
    "\n",
    "def apply_scene_flags1(scene_BT, BT_attributes):\n",
    "    \n",
    "    for i, scene in enumerate(scene_BT):\n",
    "        \n",
    "        scene_BT[i] = scene_BT[i].where(scene.qc_fov==0) # Apply 'qc_fov' flag \n",
    "        \n",
    "        # TODO: ical offsets are applied only to ssmis (they are all referenced to ssmi f11, I think, check)\n",
    "        attrs = scene_BT[i]['tb'].attrs\n",
    "        scene_BT[i]['tb'] = scene_BT[i].tb + scene_BT[i].ical # Apply intercalibration offsets         \n",
    "        attrs['long_name'] = 'brightness temperature after ical'\n",
    "        scene_BT[i]['tb'].attrs = attrs # keep attributes after ical   \n",
    "        \n",
    "        j = 0\n",
    "        for ch in scene.scene_channel.values:              # Apply 'qc_channel' flag \n",
    "            pos = (BT_attributes.qc_channel[:,scene.scene_channel[j]].values!=0)\n",
    "            scene_BT[i]['tb'].values[pos,j,:] = np.nan\n",
    "            j+=1\n",
    "        \n",
    "    return scene_BT\n",
    "\n",
    "def split_file_name(f):\n",
    "    aux1 = f.split(\".\")[0].split(\"0_\")\n",
    "    aux2 = aux1[1].split(\"_\")\n",
    "    season = aux1[0]\n",
    "    aORb = aux2[0]\n",
    "    meanORcov = aux2[1]\n",
    "    return season, aORb, meanORcov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we suppose we only care about the combined mean of each file;\n",
    "# you might also use indexing operations like .sel to subset datasets\n",
    "BT_attributes = read_netcdfs(BT_dir+BT_file+'.nc', dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_list = ['scene_env1','scene_env2'] #,'scene_img1','scene_img2','scene_las','scene_uas']\n",
    "\n",
    "scene_BT = []\n",
    "\n",
    "# BT_attributes.qc_scan can be applied at the moment of the retrieval?\n",
    "\n",
    "#for scene in scenes_list:        \n",
    "#    scene_BT.append(xr.open_mfdataset(\n",
    "#        BT_dir+BT_file, combine = 'nested', \n",
    "#        concat_dim='time', group = scene)) \n",
    "    \n",
    "# use 1 day dataset only:    \n",
    "for scene in scenes_list:        \n",
    "    scene_BT.append(xr.open_dataset(\n",
    "        BT_dir+BT_file+'.nc', group = scene))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE SCENE_HOMOGENIZATION (i.e. resample to unique or reference swath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all scenes are sampled on the same reference swath, we can concatenate the TB's\n",
    "# so we end up with a single dataset\n",
    "\n",
    "scene_BT_test = xr.concat(\n",
    "    apply_scene_flags1(scene_BT, BT_attributes),\n",
    "    dim='scene_channel').drop_vars(\n",
    "    ['laz','qc_fov','ical','eia_norm'])\n",
    "\n",
    "# Because of the way xarray.concat works \"scene_channel\" is introduced as dimension\n",
    "# in variables that do not depend on it (lat, lon, eia and sft); this is removed by\n",
    "# selecting only one \"scene_channel\" in each of those variables:\n",
    "scene_BT_test['lat'] = scene_BT_test.lat[0,:,:] #.copy()\n",
    "scene_BT_test['lon'] = scene_BT_test.lon[0,:,:] #.copy()\n",
    "scene_BT_test['eia'] = scene_BT_test.eia[0,:,:] #.copy()\n",
    "scene_BT_test['sft'] = scene_BT_test.sft[0,:,:] #.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF = scene_BT_test.assign_coords(\n",
    "    time=(BT_attributes.time)).chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_BT_test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\" #\"65536\" #\n",
    "#from dask.distributed import Client, progress, LocalCluster\n",
    "#client = Client(\n",
    "#    threads_per_worker=2, n_workers=8, processes=True)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest to user input in dataset:\n",
    "init_date = DS_CMSAF.time.sel(time=init_date, method = \"nearest\")\n",
    "end_date = DS_CMSAF.time.sel(time=end_date, method = \"nearest\")\n",
    "\n",
    "DS_CMSAF_ocean = (DS_CMSAF.sel(time=slice(init_date,end_date)\n",
    "                             , scene_channel = slice(11,13)\n",
    "                              ).where(DS_CMSAF.sft==0)).transpose(...,\"scene_channel\") \n",
    "DS_CMSAF_ocean['global_channel_ID'] = \\\n",
    "             BT_attributes.channel[DS_CMSAF_ocean.scene_channel].drop_vars('channel')\n",
    "DS_CMSAF_ocean\n",
    "delayed_obj = DS_CMSAF_ocean.to_netcdf(BT_dir+BT_file2+\".nc\", compute=False)\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "     results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = xr.open_dataset(BT_dir+BT_file2+'.nc')\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rttovStrucInit():\n",
    "    # Set knobs for use of OpenMP in RTTOV\n",
    "\n",
    "    NthreadsF = 1  # Number of threads for forward model  #os.cpu_count()\n",
    "    NthreadsFM = 4 # Number of threads for forward model (in case of multiple profiles, used in) #os.cpu_count()\n",
    "    NprofsPerCallFM = 40 # ?Roughly speaking number of variables (e.g. 279) / Number of threads (e.g. 8)\n",
    "    # Create instance of Profiles class; \n",
    "    # it's a container of the input atmospheric state that RTTOV will simulate\n",
    "\n",
    "    nprofiles = 1  # This is hardcoded (we use RTTOV within OE, workin on a single profile per OE)\n",
    "    nlev = 37 # TODO: this needs to be read from the apriori data (covariances and means) \n",
    "    myProfiles = pyrttov.Profiles(nprofiles, nlev)\n",
    "\n",
    "    #  Create instance of RTTOV\n",
    "    ssmiRttov = pyrttov.Rttov()\n",
    "\n",
    "    #chan_list_ssmi = (12,13,14,15,16,17,18) #(1,12,13,14,15,16) #  #\n",
    "\n",
    "    # Define instrument (FileCoef):\n",
    "    ssmiRttov.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "    # Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "    # so read all channels\n",
    "    try:\n",
    "        ssmiRttov.loadInst() #chan_list_ssmi\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Some settings\n",
    "    ssmiRttov.Options.AddInterp = True\n",
    "    # ssmiRttov.Options.InterpMode = 2\n",
    "    ssmiRttov.Options.CO2Data = False\n",
    "    ssmiRttov.Options.VerboseWrapper = False\n",
    "    ssmiRttov.Options.DoCheckinput = False\n",
    "    ssmiRttov.Options.UseQ2m = False\n",
    "    ssmiRttov.Options.ApplyRegLimits = True\n",
    "    ssmiRttov.Options.Verbose = False\n",
    "    ssmiRttov.Options.FastemVersion = 6\n",
    "    ssmiRttov.Options.Switchrad = True\n",
    "    ssmiRttov.Options.Nthreads = NthreadsF\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    return myProfiles, ssmiRttov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR MULTIPLE PROFILES CALL TESTING\n",
    "\n",
    "NthreadsFM = 4 # Number of threads for forward model (in case of multiple profiles, used in) #os.cpu_count()\n",
    "NprofsPerCallFM = 40 # ?Roughly speaking number of variables (e.g. 279) / Number of threads (e.g. 8)\n",
    "\n",
    "\n",
    "ssmiRttovM = pyrttov.Rttov()\n",
    "\n",
    "# SSMIS:\n",
    "\n",
    "ssmiRttovM.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "ssmiRttovM.Options.AddInterp = True\n",
    "#ssmiRttovM.Options.InterpMode = 2\n",
    "ssmiRttovM.Options.CO2Data = False\n",
    "ssmiRttovM.Options.VerboseWrapper = False\n",
    "ssmiRttovM.Options.DoCheckinput = False\n",
    "ssmiRttovM.Options.UseQ2m = False\n",
    "ssmiRttovM.Options.ApplyRegLimits = True\n",
    "ssmiRttovM.Options.Verbose = False\n",
    "ssmiRttovM.Options.FastemVersion = 6 \n",
    "ssmiRttovM.Options.Nthreads = NthreadsFM\n",
    "#ssmiRttovM.Options.NprofsPerCall = NprofsPerCallFM\n",
    "ssmiRttovM.Options.Switchrad = True\n",
    "\n",
    "# Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "# so read all channels\n",
    "try:\n",
    "    ssmiRttovM.loadInst()\n",
    "except pyrttov.RttovError as e:\n",
    "    sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF['wind'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Near surface wind speed (NSWP)',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                )\n",
    "DS_CMSAF['wind_err'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'NSWP uncertainty',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                )\n",
    "\n",
    "DS_CMSAF['chiSquareTest1'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with observation in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                )\n",
    "DS_CMSAF['chiSquareTest2'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Observation agrees with prior in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                )\n",
    "DS_CMSAF['chiSquareTest3'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with prior in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                )\n",
    "DS_CMSAF['chiSquareTest4'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with priot in X space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF = DS_CMSAF.chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t}) \n",
    "DS_CMSAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add init function:\n",
    "\n",
    "\n",
    "def init_retrieval_data(time_i, lat, lon, \n",
    "                        channels_list, BT,\n",
    "                       #dir_bands,lat_bands, \n",
    "                        zenithAngle): #, myProfiles,\n",
    "                       #ssmiRttov, ssmiRttovM=None):   \n",
    "\n",
    "    #adate = supporting_routines_m.datetime64_to_datetime(time_i)\n",
    "    #season = supporting_routines_m.date2season(adate)\n",
    "    \n",
    "    #if((lat!=np.nan)|(lon!=np.nan)):\n",
    "    \n",
    "    myProfiles, ssmiRttov = rttovStrucInit()  \n",
    "    ssmiRttovM=None\n",
    "\n",
    "    #aprioLowCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lower_cap/'\n",
    "    #aprioLowMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lowmid_cap/'\n",
    "    #aprioUpMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upmid_cap/'\n",
    "    #aprioUpCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upper_cap/'\n",
    "    \n",
    "    aprioLowCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lower_cap/'\n",
    "    aprioLowMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lowmid_cap/'\n",
    "    aprioUpMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upmid_cap/'\n",
    "    aprioUpCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upper_cap/'\n",
    "\n",
    "    # list of directories contaning covariances and means; each dir contains one geographical zone \n",
    "    # Geo. zones are divided in latitude strips: [-90,-40), [-40,0), [0,+40), [+40,+90]; lon. [-180,+180) for all\n",
    "    dir_bands = [aprioLowCapDir,aprioLowMidCapDir,aprioUpMidCapDir,aprioUpCapDir] \n",
    "    lat_bands = [[-90.0,-40.0],[-40.0,0.0],[0.0,40.0],[40.0,90.0]]\n",
    "\n",
    "    #print(nprofiles)\n",
    "    #print(time_i)\n",
    "    #print(lat)\n",
    "    #print(channel_list)\n",
    "    #print(BT)\n",
    "    #print(dir_bands)\n",
    "    #print(lat_bands)\n",
    "    #print(zenithAngle)\n",
    "    #print(myProfiles)\n",
    "    #print(ssmiRttov)\n",
    "    \n",
    "    \n",
    "    months = (time_i.astype('datetime64[M]').astype(int) % 12 + 1)%12 // 3 + 1\n",
    "    season = month2season_vec(months)\n",
    "   \n",
    "    \n",
    "    # Load Atlases: (if any)    \n",
    "    #ssmiRttov.SurfEmisRefl = np.zeros((\n",
    "    #    4, nprofiles, len(channels_list)), dtype=np.float64)\n",
    "            \n",
    "    y_obs = pd.Series(BT,\n",
    "                index=channels_list\n",
    "                     )            \n",
    "            \n",
    "    # TODO Sy needs to be loaded from somewhere according to the channels_list: ***********\n",
    "            \n",
    "    # Channels 12-16 (values from Deblonde-English 2003) (sigma or std)\n",
    "    y_noise = pd.Series(\n",
    "                    [\n",
    "                        2.4, 1.27, 1.44, #3.0, 1.34\n",
    "                    ],\n",
    "                    index=channels_list\n",
    "    )\n",
    "                            \n",
    "    # Variance values > std**2\n",
    "    S_y = pd.DataFrame(\n",
    "                np.diag(y_noise.values**2),\n",
    "                index=channels_list,\n",
    "                columns=channels_list,\n",
    "    )\n",
    "    # TODO Sy needs to be loaded from somewhere according to the channels_list  ***********\n",
    "            \n",
    "    datetime_obs = time_i #.values\n",
    "    salinity = 35 # hardcoded, not good; TODO Mario\n",
    "            \n",
    "    band = get_band(lat,dir_bands,lat_bands) # gets the directory for the band where 'lat' is located\n",
    "    xa,Sa,xb,Sb = get_mean_covs(band, season) # gets mean and covariances for the band and season\n",
    "            \n",
    "    # reverse order in x and S (profile variables are reordered from low pressure to high) \n",
    "    Sb = Sb.iloc[::-1,::-1]\n",
    "    Sa = Sa.iloc[::-1,::-1]\n",
    "    xb = xb.iloc[::-1].squeeze() # convert means from dataframe to series\n",
    "    xa = xa.iloc[::-1].squeeze()\n",
    "            \n",
    "    x_vars = xa.index.values\n",
    "    b_vars = xb.index.values\n",
    "            \n",
    "    # Get the pressure per level ; TODO: this needs to be independent of the variable (e.g. xb in this case)\n",
    "    xb_index = [float(i.split('_')[0]) for i in xb.index if i.endswith('temp')]\n",
    "    Pressure = np.array(xb_index).reshape(len(xb_index),1)\n",
    "                        \n",
    "    nlev = len([i for i in xb.index if i.endswith('temp')]) # number of levels in profile quantities\n",
    "\n",
    "\n",
    "    # forward_b_init fills \"myProfiles\" with the \"fixed\" parameters for the RTTOV simulation.\n",
    "    # The forward model F(x,b), RTTOV in our case, has two \"parameters\": x and b\n",
    "    # x is the state vector that is being retrieved (as such it is allowed to change during the retrieval)\n",
    "    # b contains all other parameters that are fixed during the retrieval (everything else that is not being retrieved)\n",
    "\n",
    "    forward_b_init(Pressure, salinity,\n",
    "                   lat, lon, datetime_obs, zenithAngle, myProfiles)\n",
    "    \n",
    "    # Define dictionary of parameters for the forward model:\n",
    "\n",
    "    forwardKwArgs = {\"myProfiles_a\" : myProfiles, \n",
    "                    \"ssmiRttov_a\" : ssmiRttov,\n",
    "                    \"channels_list\":channels_list.tolist()}    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Define dictionary of parameters for the forward model (Multiple profiles case):\n",
    "    \n",
    "    if ssmiRttovM != None:   # if an RTTOV instance for multiple profiles has been defined\n",
    "        \n",
    "        # Initialize multiple profiles for using a single call to RTTOV\n",
    "        # This is to be passed to the Jacobian function inside pyOpEst:\n",
    "        # The Jacobian is needed per parameter (len(xa.index)+len(xb.index))\n",
    "\n",
    "        nProfilesM = len(xa.index)+len(xb.index) # total number of parameters (x and b)\n",
    "            \n",
    "        # Load Atlases for Multiple profiles: (if any)    \n",
    "        #ssmiRttovM.SurfEmisRefl = np.zeros((\n",
    "        #    4, nProfilesM, len(channels_list)), dtype=np.float64) # RTTOVv12 used (2,nprof,nchan)\n",
    "                   \n",
    "        myProfilesM = pyrttov.Profiles(nProfilesM, nlev) # \n",
    "\n",
    "        #press2 = np.ones((nlev, nProfilesM))*Pressure # Pressure:(nlev,1)\n",
    "      \n",
    "        # Initialize profile datastructure for use in Jacobian computation:        \n",
    "        forward_b_init(np.ones((nlev, nProfilesM))*Pressure, salinity,\n",
    "                       lat, lon, datetime_obs, zenithAngle, myProfilesM) \n",
    "    \n",
    "        forwardKwArgsM = {\"myProfiles_a\" : myProfilesM, \n",
    "        \"ssmiRttov_a\" : ssmiRttovM,\n",
    "        \"channels_list\":channels_list.tolist()}\n",
    "    else:\n",
    "        forwardKwArgsM = None    # if not, then use only the single profile RTTOV instance;\n",
    "                                 # this results in a much slower Jacobian calculation.\n",
    "        \n",
    "        \n",
    "    # Create OE object:\n",
    "    \n",
    "    oe_ref = pyOE.optimalEstimation( # oe_1 if windDisambiguation used\n",
    "        x_vars, # state variable names\n",
    "        xa,  # a priori\n",
    "        Sa, # a priori uncertainty\n",
    "        channels_list,  # measurement variable names\n",
    "        y_obs, # observations\n",
    "        S_y, # observation uncertainty\n",
    "        forwardRT, # forward Operator\n",
    "        userJacobian=rttovK, # RTTOV's K model operator\n",
    "        forwardKwArgs=forwardKwArgs, # additonal function arguments\n",
    "        #multipleForwardKwArgs=forwardKwArgsM, # additonal function arguments for jacobian \n",
    "        #x_truth=x_truth, # true profile\n",
    "        b_vars=b_vars,   # Parameter vector variable names\n",
    "        b_p=xb,        # Parameter vector \n",
    "        S_b=Sb,        # Parameters error covariance matrix \n",
    "        perturbation=0.01,\n",
    "        verbose=False\n",
    "    )\n",
    "    return oe_ref\n",
    "\n",
    "\n",
    "#init_retrieval_data_vec = np.vectorize(init_retrieval_data, excluded=['nprofiles',\n",
    "#                                                                       'channels_list',\n",
    "#                                                                        'dir_bands',\n",
    "#                                                                        'lat_bands',\n",
    "#                                                                        'zenithAngle'\n",
    "#                                                                       ],\n",
    "#                                       otypes = [pyOE.pyOEcore.optimalEstimation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#chunk_size_time = 10\n",
    "#chunk_size_s_a_t = 10\n",
    "#DS_CMSAF_ocean = DS_CMSAF_ocean.chunk({\"time\": chunk_size_time, \n",
    "#                                       \"scene_across_track\": chunk_size_s_a_t})\n",
    "#DS_CMSAF_ocean.chunk({\"time\": 100, \n",
    "#                                       \"scene_across_track\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.full((DS_CMSAF_ocean.time.shape[0],DS_CMSAF_ocean.scene_across_track.shape[0]),np.nan)\n",
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TB dataset:\n",
    "\n",
    "DS_CMSAF_ocean_all = xr.open_dataset(BT_dir+BT_file2)\n",
    "# nearest to user input in dataset:\n",
    "init_date = DS_CMSAF_ocean_all.time.sel(time=init_date, method = \"nearest\")\n",
    "end_date = DS_CMSAF_ocean_all.time.sel(time=end_date, method = \"nearest\")\n",
    "\n",
    "DS_CMSAF_ocean = DS_CMSAF_ocean_all.sel(time=slice(init_date,end_date)\n",
    "                                       ).chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t})\n",
    "DS_CMSAF_ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month2season(month):\n",
    "    seasons = [\n",
    "        'DJF',\n",
    "        'MAM',\n",
    "        'JJA',\n",
    "        'SON',\n",
    "    ]\n",
    "    season = seasons[month-1] # month is 1 based index while seasons array needs 0 based indexing\n",
    "   \n",
    "    return season\n",
    "\n",
    "month2season_vec = np.vectorize(month2season, otypes=[str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunc(tb):\n",
    "    tb2 = tb[0]\n",
    "    return tb2\n",
    "\n",
    "def retrieveWind(time, tb, global_channel, \n",
    "                 lat, lon, eia, \n",
    "                ):\n",
    "    \n",
    "   \n",
    "    wind = np.full(lat.shape,np.nan)\n",
    "    wind_err = np.full(lat.shape,np.nan)\n",
    "\n",
    "    if(~np.isnan(lat)):  # Input dataset has been filtered for sea-land flag (so both, lat and lon are valid or nan)\n",
    "        \n",
    "        oe_ref = init_retrieval_data(time,\n",
    "                                 lat, lon,\n",
    "                                 global_channel, \n",
    "                                 tb, #dir_bands,lat_bands,\n",
    "                                 eia) #, myProfiles,\n",
    "                                 #ssmiRttov)\n",
    "    \n",
    "        #print(oe_ref)\n",
    "        oe_ref.doRetrieval()     \n",
    "    \n",
    "        if oe_ref.converged:  # \n",
    "            _, _, w10m, _, _,_ = supporting_routines_m.splitX_all_2W(oe_ref.x_op)\n",
    "            _, _, w10m_err, _, _,_ = supporting_routines_m.splitX_all_2W(oe_ref.x_op_err)\n",
    "    \n",
    "            wind =  w10m.values.item()\n",
    "            wind_err =  w10m_err.values.item()\n",
    "                \n",
    "            #chiSquareTest1 = oe_ref.chiSquareTest()[0][0]\n",
    "            #chiSquareTest2 = oe_ref.chiSquareTest()[0][1]\n",
    "            #chiSquareTest3 = oe_ref.chiSquareTest()[0][2]\n",
    "            #chiSquareTest4 = oe_ref.chiSquareTest()[0][3]\n",
    "            \n",
    "        #else:  # \n",
    "        #    wind = np.nan\n",
    "        #    wind_err = np.nan        \n",
    "            \n",
    "            #_, _, u10m, v10m, _, tsk,_ = supporting_routines_m.splitX_all_2(oe_ref.x_op)\n",
    "            #_, _, u10m_err, v10m_err, _, tsk_err,_ = supporting_routines_m.splitX_all_2(oe_ref.x_op_err)\n",
    "            \n",
    "            #w10m, w10m_err = supporting_routines_m.UV2Wvar(oe_ref.S_op.loc['00000_u10','00000_u10'],\n",
    "            #                                      oe_ref.S_op.loc['00000_v10','00000_v10'],\n",
    "            #                                      oe_ref.S_op.loc['00000_u10','00000_v10'],\n",
    "            #                                      u10m.values.item(), v10m.values.item())\n",
    "    \n",
    "    # Not over ocean:\n",
    "    #else:\n",
    "        #print(\"Lat:\")\n",
    "        #print(lat)\n",
    "        #print(\"Lon:\")\n",
    "        #print(lon)\n",
    "        #wind = np.nan\n",
    "        #wind_err = np.nan  \n",
    "        \n",
    "    return np.array([wind, wind_err,]) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\" #\"65536\" #\n",
    "#from dask.distributed import Client, progress\n",
    "#client = Client()\n",
    "#client\n",
    "DS_CMSAF_ocean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF_ocean_grouped = DS_CMSAF_ocean.groupby(\"time.hour\")\n",
    "for hour_name, hour_group in DS_CMSAF_ocean_grouped:\n",
    "    print(hour_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\" #\"65536\" #\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "    #client = Client(\n",
    "    #    threads_per_worker=2, n_workers=8, processes=True)\n",
    "    #client\n",
    "\n",
    "startTimeAll = time.time()\n",
    "\n",
    "DS_CMSAF_ocean_grouped = DS_CMSAF_ocean.groupby(\"time.hour\")\n",
    "i=0\n",
    "for hour_name, hour_group in DS_CMSAF_ocean_grouped:\n",
    "    \n",
    "    startTimeHour = time.time()\n",
    "    \n",
    "    hour_group = hour_group.chunk({\"time\": chunk_size_time,\n",
    "                                   \"scene_across_track\": chunk_size_s_a_t})\n",
    "    #client = Client(\n",
    "    #    threads_per_worker=2, n_workers=8, processes=True)\n",
    "    #client\n",
    "\n",
    "    with LocalCluster(threads_per_worker=2, \n",
    "                      n_workers=8, processes=True\n",
    "                     ) as cluster, Client(cluster) as client:\n",
    "        \n",
    "        out_wind = xr.apply_ufunc(retrieveWind, \n",
    "                              hour_group.time, \n",
    "                              hour_group.tb, \n",
    "                              hour_group.global_channel_ID, \n",
    "                              hour_group.lat, hour_group.lon, \n",
    "                              hour_group.eia, \n",
    "                              input_core_dims=[[], \n",
    "                                               [\"scene_channel\"], \n",
    "                                               [\"scene_channel\"],[],[],[],#[],[],#[],[], \n",
    "                                               #[],[],[],[] #,[],[], \n",
    "                                              ], \n",
    "                              exclude_dims= set((\"scene_channel\",)),\n",
    "                              output_core_dims=[\n",
    "                                  [\"results\"], \n",
    "                              ], \n",
    "                              dask=\"parallelized\", \n",
    "                              output_dtypes=[hour_group.wind.dtype], \n",
    "                              output_sizes={\"results\": 2},\n",
    "                              vectorize=True \n",
    "                             ).compute().chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "    print(\"%.2f s , Time_Hour\" % (time.time()-startTimeHour)) \n",
    "\n",
    "    if(i==0):\n",
    "        out_wind_1 = out_wind    \n",
    "    else:\n",
    "        out_wind_1 = xr.concat((out_wind_1, out_wind), dim = \"time\")\n",
    "\n",
    "    print(\"Time group:\")\n",
    "    print(i)\n",
    "    print(\"Done!\")\n",
    "    i+=1\n",
    "    \n",
    "DS_CMSAF_ocean[\"wind\"].data = out_wind_1.data[:,:,0]\n",
    "DS_CMSAF_ocean[\"wind_err\"].data = out_wind_1.data[:,:,1]\n",
    "\n",
    "print(\"%.2f s , TimeAll_hour\" % (time.time()-startTimeAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeAll = time.time()\n",
    "\n",
    "out_wind = xr.apply_ufunc(retrieveWind, \n",
    "                              DS_CMSAF_ocean.time, \n",
    "                              DS_CMSAF_ocean.tb, \n",
    "                              DS_CMSAF_ocean.global_channel_ID, \n",
    "                              DS_CMSAF_ocean.lat, DS_CMSAF_ocean.lon, \n",
    "                              DS_CMSAF_ocean.eia, \n",
    "                              #DS_CMSAF_ocean.wind, DS_CMSAF_ocean.wind_err, \n",
    "                              #DS_CMSAF_ocean.chiSquareTest1,DS_CMSAF_ocean.chiSquareTest2, \n",
    "                              #DS_CMSAF_ocean.chiSquareTest3, DS_CMSAF_ocean.chiSquareTest4, \n",
    "                              input_core_dims=[[], \n",
    "                                               [\"scene_channel\"], \n",
    "                                               [\"scene_channel\"],[],[],[],#[],[],#[],[], \n",
    "                                               #[],[],[],[] #,[],[], \n",
    "                                              ], \n",
    "                              exclude_dims= set((\"scene_channel\",)),\n",
    "                              output_core_dims=[#[\"time\"], \n",
    "                                  [\"results\"], \n",
    "                                  #[], \n",
    "                                  #[], \n",
    "                                  #[], \n",
    "                                  #[]], \n",
    "                              ], \n",
    "                              dask=\"parallelized\", \n",
    "                              output_dtypes=[DS_CMSAF_ocean.wind.dtype], \n",
    "                              output_sizes={\"results\": 2},\n",
    "                              vectorize=True \n",
    "                             ).compute().chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "print(\"%.2f s , TimeAll\" % (time.time()-startTimeAll)) \n",
    "\n",
    "DS_CMSAF_ocean[\"wind\"].data = out_wind.data[:,:,0]\n",
    "DS_CMSAF_ocean[\"wind_err\"].data = out_wind.data[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.diagnostics import ProgressBar\n",
    "\n",
    "# or distributed.progress when using the distributed scheduler\n",
    "delayed_obj = DS_wind.chunk(chunks={'time':10}).to_netcdf(\"DS_wind.nc\", compute=False)\n",
    "\n",
    "with ProgressBar():\n",
    "     results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wind = out_wind.chunk({\"time\": chunk_size_time, \n",
    "                                       \"scene_across_track\": chunk_size_s_a_t})\n",
    "#DS_CMSAF_ocean[\"wind\"] \n",
    "DS_CMSAF_ocean[\"wind\"].data = out_wind.data[:,:,0]\n",
    "#DS_CMSAF_ocean[\"wind_err\"] = out_wind.data[:,:,0]\n",
    "DS_CMSAF_ocean[\"wind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CMSAF_ocean[\"wind\"].data = out_wind_1.data[:,:,0]\n",
    "DS_CMSAF_ocean[\"wind_err\"].data = out_wind_1.data[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_wind_1[1000:2000,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapPlotScat(x,y,data,namefile, mini, maxi, orthoCenter=None):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    #ortho = ccrs.Orthographic(0,-15) # ccrs.Orthographic(60,-15)\n",
    "    ortho = ccrs.PlateCarree()\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    #crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.PlateCarree() #ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black',linewidth=0.1)\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.07, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER  \n",
    "    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.01, #0.15\n",
    "        marker = \"o\",\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        vmin=mini, vmax=maxi,  # 3,18\n",
    "        #vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"10m Wind Speed, RadEst [m/s]\")\n",
    "    #fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=450)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()\n",
    "\n",
    "swath_radEst = SwathDefinition(lons=DS_CMSAF_ocean.lon.values, lats=DS_CMSAF_ocean.lat.values)\n",
    "lons_radEst, lats_radEst = swath_radEst.get_lonlats()\n",
    "\n",
    "world_lons, world_lats, world_wind_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.wind.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_err_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.wind_err.values,\n",
    "                            radius_of_influence=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_radEst,\n",
    "                 'World_wind_RadEst_Plat', 3,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_err_radEst,\n",
    "                 'World_wind_Err_RadEst_Plat',0.5,1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wind_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_TB_frame(BT_scene_env1, area_interest, begin_t, end_t):\n",
    "    \n",
    "    lat_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lat[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    lon_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lon[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    tb_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.tb[begin_t:end_t,:,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "\n",
    "    grid_lons_interest, grid_lats_interest = area_interest.get_lonlats()\n",
    "\n",
    "    swath_scene1 = SwathDefinition(lons=lon_scene1, lats=lat_scene1)\n",
    "    lons_scene1, lats_scene1 = swath_scene1.get_lonlats()\n",
    "\n",
    "    reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "\n",
    "    return reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1\n",
    "    #swath_reduced_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def defineArea(corners, proj_id, datum):\n",
    "    #corners=parseMeta(data_name)\n",
    "\n",
    "    lat_0 = '{lat_0:5.2f}'.format_map(corners)\n",
    "    lon_0= '{lon_0:5.2f}'.format_map(corners)\n",
    "    lon_bbox = [corners['min_lon'],corners['max_lon']]\n",
    "    lat_bbox = [corners['min_lat'],corners['max_lat']]\n",
    "#    area_dict = dict(datum=datum,lat_0=lat_0,lon_0=lon_0,\n",
    "#                proj=proj_id,units='m')\n",
    "\n",
    "    area_dict = dict(datum=datum,lat_0=-15,lon_0=60,\n",
    "                proj=proj_id,units='m',a=6370997.0,)\n",
    "\n",
    "    prj=pyproj.Proj(area_dict)\n",
    "    x, y = prj(lon_bbox, lat_bbox)\n",
    "    xsize=200\n",
    "    ysize=200\n",
    "    area_id = 'granule'\n",
    "    area_name = 'modis swath 5min granule'\n",
    "    area_extent = (x[0], y[0], x[1], y[1])\n",
    "    print(area_extent)\n",
    "    area_def = AreaDefinition(area_id, area_name, proj_id, \n",
    "                                   area_dict, xsize, ysize,area_extent)\n",
    "    return area_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation of area of interest:\n",
    "#corners = {\"min_lon\": 25 , \"max_lon\": 75, \"min_lat\": -30 , \"max_lat\": 0, \"lat_0\": 60, \"lon_0\":-15}\n",
    "corners = {\"min_lon\": -95 , \"max_lon\": 20, \"min_lat\": 3 , \"max_lat\": 50, \"lat_0\": 27, \"lon_0\":-57}\n",
    "proj_id = 'eqc'  # eqc\n",
    "datum = 'WGS84'\n",
    "area_interest = defineArea(corners, proj_id, datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeMapAnimScat(BT_scene, BT_attributes, channel, area, \n",
    "                      init_date, nFrames, delta_hours, namefile):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(frameon=False) #figsize=(8, 6))\n",
    "    fig.add_axes([0,0,1,1])\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    ax.set_title(\"TB \"+namefile)\n",
    "    #ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black') \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "\n",
    "\n",
    "    #delta_hours = 12\n",
    "    end_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "    time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "        BT_attributes.time.values<end_date))\n",
    "    begin_t = time_slice[0][0]  \n",
    "    end_t = time_slice[0][-1]\n",
    "\n",
    "    x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "    \n",
    "    ims = []\n",
    "    im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12     # 180, 270\n",
    "            vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    \n",
    "    #fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    \n",
    "    for i in np.arange(nFrames):\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12  # 180, 270\n",
    "            vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "        \n",
    "        ims.append([im1])\n",
    "        init_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "        end_date = end_date + np.timedelta64(delta_hours, 'h') \n",
    "        time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "            BT_attributes.time.values<end_date))\n",
    "        begin_t = time_slice[0][0]  \n",
    "        end_t = time_slice[0][-1] \n",
    "        x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    plt.tight_layout()\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 12\n",
    "delta_hours = 12\n",
    "channel = 2\n",
    "namefile = 'env1_22V_12h_F16'\n",
    "timeMapAnimScat(BT_scene, BT_attributes, channel, area_interest, \n",
    "                      init_date, nFrames, delta_hours, namefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath scene 1 at a world wide scale:\n",
    "\n",
    "result_scene1 = resample_nearest(swath_scene1, tb_scene1.values, area_def_world, \n",
    "                          radius_of_influence=30000, fill_value=np.nan)\n",
    "\n",
    "#result_scene2 = resample_nearest(swath_scene2, tb_scene2.values, area_def_world, \n",
    "#                          radius_of_influence=30000, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "\n",
    "\n",
    "#reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2 = \\\n",
    "#                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "#                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "#                            radius_of_influence=3000)\n",
    "#swath_reduced_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "world_lons_scene1, world_lats_scene1, world_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)\n",
    "\n",
    "world_lons_scene2, world_lats_scene2, world_data_scene2 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath data into a grid in the area of interest \n",
    "result_reduced_scene1 = resample_nearest(swath_reduced_scene1, reduced_data_scene1, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)\n",
    "\n",
    "result_reduced_scene2 = resample_nearest(swath_reduced_scene2, reduced_data_scene2, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapArea('mapArea0', area_def_world)\n",
    "mapArea('mapArea01', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chann = 0\n",
    "# Plot resampled (grid version) scenes:\n",
    "basicMapPlot(result_scene1[:,:,chann],'scene1'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene1[:,:,chann],\n",
    "             'scene1_reduced'+str(chann), area_interest)  # map only the area of interest, grid\n",
    "\n",
    "basicMapPlot(result_scene2[:,:,chann],'scene2'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene2[:,:,chann],\n",
    "             'scene2_reduced'+str(chann), area_interest)  # map only the area of interest, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in area of interest, Plate Carree projection\n",
    "\n",
    "chann = 0\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19H', area_interest, )\n",
    "chann = 1\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19V', area_interest, )\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_22V', area_interest, )\n",
    "#chann = 1\n",
    "#basicMapPlotScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene2_scatt_PCarr_91V', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path('/nobackup/users/echeverr/fortran_tests/netcdf/following_Edouard/imgs/ch19H')\n",
    "images = list(image_path.glob('*.png'))\n",
    "image_list = []\n",
    "for file_name in images:\n",
    "    image_list.append(imageio.imread(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimwrite('animated_from_images.gif', image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in world area, Orthographic projection\n",
    "\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat1(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'world_scene1_scatt_Orth_22V', area_interest)\n",
    "#chann = 3\n",
    "#basicMapPlotScat1(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'world_scene1_scatt_Orth_91H', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot original swath pixels:\n",
    "chann = 2\n",
    "basicMapPlotScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'scene1_scatt_Orth_world_22V', area_def_world)\n",
    "chann = 3\n",
    "basicMapPlotScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'scene1_scatt_Orth_world_91H', area_def_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlot(result,namefile, area):\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    #ax.background_img(name='BM', resolution='high') \n",
    "    ax.coastlines();\n",
    "    #ax.stock_img();\n",
    "    ax.grid(True)\n",
    "    #ax.set_xlabel('Longitude [deg]')\n",
    "    #ax.set_ylabel('Latitude [deg]')\n",
    "\n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    #gl.xlabels_top = False\n",
    "    #gl.ylabels_left = False\n",
    "    #gl.ylabels_right=True\n",
    "    #gl.xlines = True\n",
    "    #gl.xlocator = mticker.FixedLocator([70, 75, 80, 85])\n",
    "    #gl.ylocator = mticker.FixedLocator([-5, -3, -1, 1, 3])\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    im = ax.imshow(result, transform=crs, extent=crs.bounds, origin='upper', cmap='jet', vmin=150, vmax=250)\n",
    "    fig.colorbar(im,ax=ax) \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)   \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "    ax.set_title(\"TB\")\n",
    "    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        #vmin=3, vmax=18         #180, 270\n",
    "        vmin=130, vmax=270         #180, 270\n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"Brightness temperature [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapArea(namefile, area):\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    \n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "    \n",
    "    ax.coastlines(linewidth=0.5)   \n",
    "    #ax.set_global() \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chann = 0\n",
    "#basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene1_scatt'+str(chann), area_interest)\n",
    "\n",
    "nFrames = 150\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    #ax.set_title(\"TB \"+namefile)\n",
    "    ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "   \n",
    "    #gl0 = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl0.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl0.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = x[start_:end_]\n",
    "    y2 = y[start_:end_]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12     # 180, 270\n",
    "            #vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12  # 180, 270\n",
    "            #vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,x[start_:end_])\n",
    "        y2 = np.append(y2,y[start_:end_])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'_bar.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BT_scene_env2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat1(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ortho = ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    \n",
    "    #ax.set_title(\"TB\")\n",
    "    #ax.coastlines() \n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        #vmin=3, vmax=18,  # 180, 270\n",
    "        vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    #fig.colorbar(im).set_label(\"10m Wind Speed, HOAPS [m/s]\")\n",
    "    fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 1200\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat1(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    ortho = ccrs.Orthographic(-39,18) #ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "       \n",
    "    ax.set_title(\"Wind Speed \"+namefile) \n",
    "    #ax.set_title(\"Temperature Brightness \"+namefile) \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = xy[start_:end_,0]\n",
    "    y2 = xy[start_:end_,1]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18  # 180, 250\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18   # 180, 270 TB\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,xy[start_:end_,0])\n",
    "        y2 = np.append(y2,xy[start_:end_,1])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=30, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
