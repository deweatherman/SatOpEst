{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#import ftplib\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime \n",
    "import time\n",
    "from datetime import datetime \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import scipy.stats as stats\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "##import sklearn\n",
    "#from sklearn.metrics import median_absolute_error, mean_squared_error,r2_score\n",
    "#from sklearn.linear_model import LinearRegression, RANSACRegressor, HuberRegressor\n",
    "\n",
    "\n",
    "import pyOptimalEstimation as pyOE\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cmocean import cm as cmo\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from glob import glob\n",
    "\n",
    "#import imageio\n",
    "\n",
    "#from pathlib import Path\n",
    "\n",
    "#from netCDF4 import Dataset\n",
    "\n",
    "import gc, psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "import pyresample\n",
    "from pyresample import create_area_def, load_area, data_reduce, utils, AreaDefinition\n",
    "from pyresample.geometry import SwathDefinition\n",
    "from pyresample.kd_tree import resample_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sys.path.append('/nobackup/users/echeverr/Git/SatOpEst/support') # where supporting_routines_m live\n",
    "sys.path.append('/home/mario/Documents/work/code/git/SatOpEst/support') # where supporting_routines_m live\n",
    "\n",
    "import supporting_routines_m \n",
    "\n",
    "import os\n",
    "\n",
    "#rttov_installdir = '/usr/people/echeverr/Documents/code/nwpsaf/rttov13'\n",
    "rttov_installdir = '/home/mario/myLibs/rrtov13/rttov130'\n",
    "\n",
    "sys.path.append(rttov_installdir+'/wrapper')\n",
    "import pyrttov\n",
    "\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "#os.environ[\"CARTOPY_USER_BACKGROUNDS\"] = os.path.join(current_directory,'/nobackup/users/echeverr/py_tests/earthpy_example/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = psutil.Process()\n",
    "\n",
    "#BT_dir = '/nobackup/users/echeverr/data/cmsaf/ssmis/F16/'\n",
    "BT_dir = '/home/mario/Data/CMSAF/ssims/F16/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BT_file = '*.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions for the apriori means and covariances directories:\n",
    "\n",
    "#aprioLowCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/SurfaceParamsEra5_2011_2014/log_gHum/lower_cap/'\n",
    "#aprioLowMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/SurfaceParamsEra5_2011_2014/log_gHum/lowmid_cap/'\n",
    "#aprioUpMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/SurfaceParamsEra5_2011_2014/log_gHum/upmid_cap/'\n",
    "#aprioUpCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/SurfaceParamsEra5_2011_2014/log_gHum/upper_cap/'\n",
    "\n",
    "aprioLowCapDir = '/home/mario/Data/Covariance_means/log_gHum/lower_cap/'\n",
    "aprioLowMidCapDir = '/home/mario/Data/Covariance_means/log_gHum/lowmid_cap/'\n",
    "aprioUpMidCapDir = '/home/mario/Data/Covariance_means/log_gHum/upmid_cap/'\n",
    "aprioUpCapDir = '/home/mario/Data/Covariance_means/log_gHum/upper_cap/'\n",
    "\n",
    "\n",
    "# list of directories contaning covariances and means; each dir contains one geographical zone \n",
    "# Geo. zones are divided in latitude strips: [-90,-40), [-40,0), [0,+40), [+40,+90]; lon. [-180,+180) for all\n",
    "dir_bands = [aprioLowCapDir,aprioLowMidCapDir,aprioUpMidCapDir,aprioUpCapDir] \n",
    "lat_bands = [[-90.0,-40.0],[-40.0,0.0],[0.0,40.0],[40.0,90.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Netcdf using xarray:\n",
    "def read_netcdfs(files, dim, transform_func=None, groups = None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "        with xr.open_dataset(path, group = groups) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "            ds.load()\n",
    "            return ds\n",
    "\n",
    "    paths = sorted(glob(files))\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# returns the directory (str) corrisponding to the band where \"lat\" is located\n",
    "def get_band(lat,dir_bands,lat_bands):\n",
    "    i=0\n",
    "    for band in dir_bands: \n",
    "        if(np.logical_and(lat >= lat_bands[i][0], lat < lat_bands[i][1])):\n",
    "            band_out =band\n",
    "        i+=1\n",
    "    return band_out\n",
    "\n",
    "# Apply usual flags to CMSAF dataset (per scene)\n",
    "# Author: M. Echeverri, March 2021.\n",
    "# TODO:\n",
    "# - Add list of flags that the user wants to apply\n",
    "# - Add input checks\n",
    "\n",
    "def apply_scene_flags(scene_BT, BT_attributes):\n",
    "    \n",
    "    for i, scene in enumerate(scene_BT):\n",
    "        #scene_aux = scene\n",
    "        scene_aux = xr.where((scene.qc_fov==0), \n",
    "                                      scene, \n",
    "                                      np.nan)    # Apply 'qc_fov' flag \n",
    "        \n",
    "        scene_aux[\"tb\"] = xr.where((scene[\"tb\"]!=np.nan), \n",
    "                                      (scene[\"tb\"] + scene[\"ical\"]), \n",
    "                                          np.nan)  # Apply intercalibration offsets         \n",
    "\n",
    "        j = 0\n",
    "        for ch in scene.scene_channel.values:              # Apply 'qc_channel' flag \n",
    "            pos = (BT_attributes.qc_channel[:,scene.scene_channel[j]].values!=0)\n",
    "            scene_aux[\"tb\"][pos,j,:] = np.nan\n",
    "            j+=1\n",
    "\n",
    "        scene_BT[i] = scene_aux    \n",
    "    return scene_BT\n",
    "\n",
    "\n",
    "def apply_scene_flags1(scene_BT, BT_attributes):\n",
    "    \n",
    "    for i, scene in enumerate(scene_BT):\n",
    "        \n",
    "        scene_BT[i] = scene_BT[i].where(scene.qc_fov==0) # Apply 'qc_fov' flag \n",
    "        \n",
    "        # TODO: ical offsets are applied only to ssmis (they are all referenced to ssmi f11, I think, check)\n",
    "        attrs = scene_BT[i]['tb'].attrs\n",
    "        scene_BT[i]['tb'] = scene_BT[i].tb + scene_BT[i].ical # Apply intercalibration offsets         \n",
    "        attrs['long_name'] = 'brightness temperature after ical'\n",
    "        scene_BT[i]['tb'].attrs = attrs # keep attributes after ical   \n",
    "        \n",
    "        j = 0\n",
    "        for ch in scene.scene_channel.values:              # Apply 'qc_channel' flag \n",
    "            pos = (BT_attributes.qc_channel[:,scene.scene_channel[j]].values!=0)\n",
    "            scene_BT[i]['tb'].values[pos,j,:] = np.nan\n",
    "            j+=1\n",
    "        \n",
    "    return scene_BT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_file_name(f):\n",
    "    aux1 = f.split(\".\")[0].split(\"0_\")\n",
    "    aux2 = aux1[1].split(\"_\")\n",
    "    season = aux1[0]\n",
    "    aORb = aux2[0]\n",
    "    meanORcov = aux2[1]\n",
    "    return season, aORb, meanORcov\n",
    "\n",
    "def get_mean_covs(directory,inputSeason):\n",
    "# very badly programed look-up table: it will look\n",
    "# for the files corrisponding to a given inputSeason \n",
    "# and return means and covariances for both state vector (a)\n",
    "# and parameters vector (b).\n",
    "    files = os.listdir(directory)\n",
    "    for f in files:\n",
    "        #print(f)\n",
    "        season, aORb, meanORcov = split_file_name(f)\n",
    "        if(inputSeason==season):\n",
    "            if(aORb=='a'):\n",
    "                if(meanORcov=='mean'):\n",
    "                    xa = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                elif(meanORcov=='cov'):   \n",
    "                    Sa = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                else:\n",
    "                    print('Something went wrong, check:')\n",
    "                    print(directory+f)\n",
    "            elif(aORb=='b'):\n",
    "                if(meanORcov=='mean'):\n",
    "                    xb = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                elif(meanORcov=='cov'):   \n",
    "                    Sb = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                else:\n",
    "                    print('Something went wrong, check:')\n",
    "                    print(directory+f)                \n",
    "            \n",
    "        \n",
    "    return xa,Sa,xb,Sb   \n",
    "\n",
    "def reshape4profiles(profiles):\n",
    "    # \"profiles\" is a numpy array\n",
    "    # \"profiles\" can contain 1 or more profiles\n",
    "    # \"profiles\" has dimensions (nlevels, nprofiles)\n",
    "    # \"outProfiles\" has dimensions (nprofiles,nlevels) (as needed in RTTOV)\n",
    "    \n",
    "    if (len(profiles.shape)==1):\n",
    "        outProfiles = profiles.reshape(1,profiles.shape[0]).copy()\n",
    "    else:\n",
    "        outProfiles = profiles.T.copy() #profiles.reshape(profiles.shape[1]\n",
    "            #                          ,profiles.shape[0]) \n",
    "    return outProfiles  \n",
    "\n",
    "def expand2nprofiles(n, nprof):\n",
    "    # Transform 1D array to a [nprof, nlevels] array\n",
    "    outp = np.empty((nprof, len(n)), dtype=n.dtype)\n",
    "    for i in range(nprof):\n",
    "        outp[i, :] = n[:]\n",
    "    return outp\n",
    "\n",
    "def forward_b_init(pressure, salinity, lat, long, datetime_obs64, \n",
    "                   zenithAngle, myProfiles):\n",
    "    \n",
    "    if (len(pressure.shape)==1):\n",
    "        nprofiles = 1\n",
    "    else:\n",
    "        nprofiles =  pressure.shape[1]\n",
    "    \n",
    "    # The rest of the code uses datetime64 format (numpy), but I have to pass the obs date as integers to RTTOV\n",
    "    datetime_obs = supporting_routines_m.datetime64_to_datetime(datetime_obs64)\n",
    "    \n",
    "    s2m = np.zeros((nprofiles,6), dtype=np.float64) # s2m has 6 elements (docs RTTOV)\n",
    "    \n",
    "    angles = np.zeros((nprofiles,4), dtype=np.float64) # angles has 4 elements (docs RTTOV)\n",
    "    angles[:,0] = zenithAngle\n",
    "    \n",
    "    \n",
    "    # for RTTOV 13 skin is 9 elements long:\n",
    "    skin = np.zeros((nprofiles,9), dtype=np.float64) # skin has 9 elements (docs RTTOV)\n",
    "    skin[:,1] = salinity\n",
    "        \n",
    "    surftype = np.zeros((nprofiles,2), dtype=np.int32) # surftype has 2 elements (docs RTTOV)\n",
    "    surftype[:,:] = 1 # [sea, ocean] Harcoded for now, TODO *** mario\n",
    "    \n",
    "    \n",
    "    surfgeom = np.zeros((nprofiles,3), dtype=np.float64) # surfgeom has 3 elements (docs RTTOV)\n",
    "    surfgeom[:,0] = lat\n",
    "    surfgeom[:,1] = long\n",
    "    # surfgeom[:,2]=0 # elevation harcoded to 0 for now, TODO *** mario\n",
    "    \n",
    "    date_times = np.zeros((nprofiles,6), dtype=np.int32) # date_times has 6 elements (docs RTTOV)\n",
    "    date_times[:,0] = datetime_obs.year\n",
    "    date_times[:,1] = datetime_obs.month\n",
    "    date_times[:,2] = datetime_obs.day\n",
    "    date_times[:,3] = datetime_obs.hour\n",
    "    date_times[:,4] = datetime_obs.minute\n",
    "    date_times[:,5] = datetime_obs.second\n",
    "    \n",
    "    \n",
    "    myProfiles.GasUnits = 1  # kg/kg (see RTTOV doc. for other options) # Harcoded for now, TODO *** mario\n",
    "    myProfiles.P = reshape4profiles(pressure) \n",
    "    myProfiles.S2m = s2m\n",
    "    myProfiles.Angles = angles\n",
    "    myProfiles.Skin = skin\n",
    "    myProfiles.SurfType = surftype\n",
    "    myProfiles.SurfGeom = surfgeom\n",
    "    myProfiles.DateTimes = date_times\n",
    "    \n",
    "    \n",
    "def forwardRT1(X, myProfiles, ssmiRttov, channels_list=None):\n",
    "    \n",
    "    # TODO: Add assertions, tests *** mario\n",
    "    \n",
    "    # if wind speed in components:\n",
    "\n",
    "    temperature, humidity, u10m, v10m, t2m, tsk, sp = supporting_routines_m.splitX_all_2(X)\n",
    "\n",
    "    \n",
    "    # humdity is in [g/kg, log10] scale, convert to linear in kg/kg\n",
    "    humidity = (10**humidity) / 1000.\n",
    "    # or abs_humidity? *** note mario\n",
    "    \n",
    "    myProfiles.T = expand2nprofiles(np.array(temperature.values,dtype=np.float64), nprofiles)\n",
    "    myProfiles.Q = expand2nprofiles(np.array(humidity.values,dtype=np.float64), nprofiles)  \n",
    "\n",
    "    myProfiles.S2m[0,0] = np.float64(sp.values) # surface pressure\n",
    "    \n",
    "    myProfiles.S2m[0,1] = np.float64(t2m.values) # 2m temperature\n",
    "    myProfiles.Skin[0,0] = np.float64(tsk.values)\n",
    "    \n",
    "    #myProfiles.S2m[0,3] = np.float64(wind10m.values) / ( np.sqrt(2) ) #  10m windspeed, u component\n",
    "    #myProfiles.S2m[0,4] = np.float64(wind10m.values) / ( np.sqrt(2) ) #  10m windspeed, v component\n",
    "    \n",
    "    # if wind in components:\n",
    "    myProfiles.S2m[0,3] = np.float64(u10m.values) #  10m windspeed, u component\n",
    "    myProfiles.S2m[0,4] = np.float64(v10m.values) #  10m windspeed, v component     \n",
    "       \n",
    "    ssmiRttov.Profiles = myProfiles\n",
    "    \n",
    "    ssmiRttov.SurfEmisRefl[:,:,:] = -1. # need to \"reset\" to -1 every time RTTOV is called; \n",
    "    # -1 indicates to RTTOV to use internal values for surface emissivity.\n",
    "    \n",
    "    try:\n",
    "        ssmiRttov.runDirect(channels_list)\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error running RTTOV direct model: {!s}\".format(e))\n",
    "        sys.exit(1)    \n",
    "        \n",
    "    TB = ssmiRttov.BtRefl[0, :]\n",
    "    \n",
    "    return TB    \n",
    "\n",
    "\n",
    "def forwardRT(X, myProfiles_a, ssmiRttov_a, channels_list=None):\n",
    "    \n",
    "    # TODO: Add assertions, tests *** mario\n",
    "\n",
    "    # X contains T, Q and W10, lets split the vector\n",
    "    #temperature, humidity, wind10m = supporting_routines_m.splitX(X)\n",
    "    \n",
    "    # if wind speed in components:\n",
    "\n",
    "    #temperature, humidity, u10m, v10m, bp2m, bt2m, btsk \\\n",
    "    #= supporting_routines_m.splitX_all(X)\n",
    "    \n",
    "    #NEW\n",
    "    temperature, humidity, u10m, v10m, t2m, tsk, sp \\\n",
    "    = supporting_routines_m.splitX_all_2(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # humdity is in log10 scale, convert to linear in kg/kg\n",
    "    humidity = (10**humidity) / 1000.\n",
    "    # or abs_humidity? *** note mario\n",
    "\n",
    "    myProfiles_a.T = reshape4profiles(temperature.to_numpy(dtype=np.float64))  \n",
    "    myProfiles_a.Q = reshape4profiles(humidity.to_numpy(dtype=np.float64))  \n",
    "\n",
    "    myProfiles_a.S2m[:,0] = reshape4profiles(\n",
    "        sp.to_numpy(dtype=np.float64)).flatten() # surface pressure\n",
    "    myProfiles_a.S2m[:,1] = reshape4profiles(\n",
    "        t2m.to_numpy(dtype=np.float64)).flatten()  # 2m temperature\n",
    "    myProfiles_a.Skin[:,0] = reshape4profiles(\n",
    "        tsk.to_numpy(dtype=np.float64)).flatten() \n",
    "    \n",
    "    # if wind in components:\n",
    "    myProfiles_a.S2m[:,3] = reshape4profiles(\n",
    "        u10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, u component\n",
    "    myProfiles_a.S2m[:,4] = reshape4profiles(\n",
    "        v10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, v component  \n",
    "    \n",
    "   \n",
    "    ssmiRttov_a.Profiles = myProfiles_a\n",
    "    \n",
    "    ssmiRttov_a.SurfEmisRefl[:,:,:] = -1. # need to \"reset\" to -1 every time RTTOV is called; \n",
    "    # -1 indicates to RTTOV to use internal values for surface emissivity.\n",
    "\n",
    "    try:\n",
    "        ssmiRttov_a.runDirect(channels_list)\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error running RTTOV direct model: {!s}\".format(e))\n",
    "        sys.exit(1)    \n",
    "        \n",
    "    #print(ssmiRttov.BtRefl[:, :].shape)\n",
    "    #print(ssmiRttov.BtRefl[:, :])\n",
    "    \n",
    "    if(ssmiRttov_a.BtRefl[:, :].shape[0]==1):\n",
    "        TB = ssmiRttov_a.BtRefl[0, :].T\n",
    "    else:\n",
    "        TB = ssmiRttov_a.BtRefl[:, :].T\n",
    "    \n",
    "    return TB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sleep_and_print_mem(title, sleep=3):\n",
    "    time.sleep(sleep)\n",
    "    print(\"\\n\" + title + \" : \" +  \"%0.2f MB\" % (p.memory_info().rss / 1e6)\n",
    "         + \"   \" + \"=\" * 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# here we suppose we only care about the combined mean of each file;\n",
    "# you might also use indexing operations like .sel to subset datasets\n",
    "BT_attributes = read_netcdfs(BT_dir+BT_file, dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_list = ['scene_env1','scene_env2'] #,'scene_img1','scene_img2','scene_las','scene_uas']\n",
    "\n",
    "scene_BT = []\n",
    "\n",
    "# BT_attributes.qc_scan can be applied at the moment of the retrieval?\n",
    "\n",
    "for scene in scenes_list:        \n",
    "    scene_BT.append(xr.open_mfdataset(\n",
    "        BT_dir+BT_file, combine = 'nested', \n",
    "        concat_dim='time', group = scene)) #.where(\n",
    "        #BT_attributes.qc_scan==0)) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE SCENE_HOMOGENIZATION (i.e. resample to unique or reference swath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.count_nonzero(BT_attributes.qc_scan==True)\n",
    "#aa = xr.open_dataset(BT_dir+'BTRin20140909000000324SSF1701GL.nc', group = 'scene_env1')\n",
    "#BT_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all scenes are sampled on the same reference swath, we can concatenate the TB's\n",
    "# so we end up with a single dataset\n",
    "\n",
    "scene_BT_test = xr.concat(\n",
    "    apply_scene_flags1(scene_BT, BT_attributes),\n",
    "    dim='scene_channel').drop_vars(\n",
    "    ['laz','qc_fov','ical','eia_norm'])\n",
    "\n",
    "# Because of the way xarray.concat works \"scene_channel\" is introduced as dimension\n",
    "# in variables that do not depend on it (lat, lon, eia and sft); this is removed by\n",
    "# selecting only one \"scene_channel\" in each of those variables:\n",
    "scene_BT_test['lat'] = scene_BT_test.lat[0,:,:] #.copy()\n",
    "scene_BT_test['lon'] = scene_BT_test.lon[0,:,:] #.copy()\n",
    "scene_BT_test['eia'] = scene_BT_test.eia[0,:,:] #.copy()\n",
    "scene_BT_test['sft'] = scene_BT_test.sft[0,:,:] #.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_BT_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset where we remove the observations over land (where sft == 1):\n",
    "#DS_CMSAF = scene_BT_test.where(\n",
    "#    scene_BT_test.sft==0).dropna(dim='time',how='all').copy()\n",
    "\n",
    "DS_CMSAF = scene_BT_test.assign_coords(\n",
    "    time=(BT_attributes.time)).where(\n",
    "    scene_BT_test.sft==0).dropna(\n",
    "    dim='time',how='all').chunk({\"time\":100, \"scene_channel\":None, \"scene_across_track\":None}) #.copy()\n",
    "\n",
    "\n",
    "# free memory\n",
    "#scene_BT_test =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_BT_test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(DS_CMSAF.sft.values==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# Create instance of Profiles class; \n",
    "# it's a container of the input atmospheric state that RTTOV will simulate\n",
    "\n",
    "nprofiles = 1  # This is hardcoded (we use RTTOV within OE, workin on a single profile per OE)\n",
    "nlev = 37 # TODO: this needs to be read from the apriori data (covariances and means) \n",
    "myProfiles = pyrttov.Profiles(nprofiles, nlev)\n",
    "\n",
    "# Create instance of RTTOV\n",
    "ssmiRttov = pyrttov.Rttov()\n",
    "\n",
    "#chan_list_ssmi = (12,13,14,15,16,17,18) #(1,12,13,14,15,16) #  #\n",
    "\n",
    "# Define instrument (FileCoef):\n",
    "ssmiRttov.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "# Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "# so read all channels\n",
    "try:\n",
    "    ssmiRttov.loadInst() #chan_list_ssmi\n",
    "except pyrttov.RttovError as e:\n",
    "    sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Some settings\n",
    "ssmiRttov.Options.AddInterp = True\n",
    "ssmiRttov.Options.CO2Data = False\n",
    "ssmiRttov.Options.VerboseWrapper = True\n",
    "ssmiRttov.Options.DoCheckinput = False\n",
    "ssmiRttov.Options.UseQ2m = False\n",
    "ssmiRttov.Options.ApplyRegLimits = True\n",
    "ssmiRttov.Options.Verbose = True\n",
    "ssmiRttov.Options.FastemVersion = 6     \n",
    "\n",
    "# *********************************\n",
    "# *********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR MULTIPLE PROFILES CALL TESTING\n",
    "\n",
    "ssmiRttovM = pyrttov.Rttov()\n",
    "\n",
    "# SSMIS:\n",
    "\n",
    "ssmiRttovM.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "ssmiRttovM.Options.AddInterp = True\n",
    "ssmiRttovM.Options.CO2Data = False\n",
    "ssmiRttovM.Options.VerboseWrapper = True\n",
    "ssmiRttovM.Options.DoCheckinput = False\n",
    "ssmiRttovM.Options.UseQ2m = True\n",
    "ssmiRttovM.Options.ApplyRegLimits = True\n",
    "ssmiRttovM.Options.Verbose = False\n",
    "ssmiRttovM.Options.FastemVersion = 6 \n",
    "\n",
    "# Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "# so read all channels\n",
    "try:\n",
    "    ssmiRttovM.loadInst()\n",
    "except pyrttov.RttovError as e:\n",
    "    sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user input:\n",
    "init_date = np.datetime64('2014-09-09T00:00:00.000') \n",
    "end_date = np.datetime64('2014-09-09T00:03:00.000') \n",
    "\n",
    "# nearest to user input in dataset:\n",
    "init_date = DS_CMSAF.time.sel(time=init_date, method = \"nearest\")\n",
    "end_date = DS_CMSAF.time.sel(time=end_date, method = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = ((~np.isnan(DS_CMSAF.lat.sel(time=slice(init_date,end_date)).values))& # check for valid (lat,lon) (i.e. over the ocean) \n",
    "          (~np.isnan(DS_CMSAF.lon.sel(time=slice(init_date,end_date)).values)))\n",
    "total_count = int(aa.sum()/(90))\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_wind = DS_CMSAF.sel(time=slice(init_date,end_date)).copy().drop_vars(['eia','sft','tb','scene_channel'])\n",
    "DS_wind['wind'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Near surface wind speed (NSWP)',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                )\n",
    "DS_wind['wind_err'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF.time.shape[0],DS_CMSAF.scene_across_track.shape[0]),np.nan),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF.time, 'scene_across_track':DS_CMSAF.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'NSWP uncertainty',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min lat')\n",
    "print(np.nanmin((DS_CMSAF.lat.sel(time=slice(init_date,end_date)).values)))\n",
    "print('max lat')\n",
    "print(np.nanmax((DS_CMSAF.lat.sel(time=slice(init_date,end_date)).values)))\n",
    "print('max lon')\n",
    "print(np.nanmax((DS_CMSAF.lon.sel(time=slice(init_date,end_date)).values)))\n",
    "print('min lon')\n",
    "print(np.nanmin((DS_CMSAF.lon.sel(time=slice(init_date,end_date)).values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sleep_and_print_mem(\"Initial memory\")            \n",
    "\n",
    "startTimeAll = time.time()\n",
    "\n",
    "# Main cycle: loop through locations in the swath\n",
    "count = 0\n",
    "#for time in DS_CMSAF.time.loc[init_date:end_date]: #DS_CMSAF.time[0:3]:\n",
    "\n",
    "i = 0\n",
    "\n",
    "for time_i in DS_CMSAF.time.sel(time=slice(init_date,end_date)): #DS_CMSAF.time[0:3]:\n",
    "    adate = supporting_routines_m.datetime64_to_datetime(time_i.values)\n",
    "    season = supporting_routines_m.date2season(adate)\n",
    "    \n",
    "    for acrossT in DS_CMSAF.scene_across_track[0:90]:\n",
    "        if(~np.isnan(DS_CMSAF.lat.loc[time_i,acrossT].values)& # check for valid (lat,lon) (i.e. over the ocean) \n",
    "          ~np.isnan(DS_CMSAF.lon.loc[time_i,acrossT].values)):\n",
    "                   \n",
    "                \n",
    "            startTimeRest = time.time()\n",
    "            \n",
    "            i+=1    \n",
    "                \n",
    "            startTimeGetVars = time.time() \n",
    "            \n",
    "            lat = DS_CMSAF.lat.loc[\n",
    "                time_i,acrossT].values.item() # expected to be a scalar; TODO add check\n",
    "            lon = DS_CMSAF.lon.loc[\n",
    "                time_i,acrossT].values.item() # expected to be a scalar; TODO add check\n",
    "            \n",
    "            #  TODO add checks on shapes or sizes of the next 3 arrays:\n",
    "            valid_channels = np.where(~np.isnan(  # not nan channels only\n",
    "                DS_CMSAF.tb.loc[time_i,:,acrossT].values))\n",
    "            \n",
    "            channels_list =  BT_attributes.channel[ # global channels ID's\n",
    "                DS_CMSAF.scene_channel.values[valid_channels]].values[0:7] # TODO: [0:7] is hardcoded, not good\n",
    "            \n",
    "            \n",
    "            BT = DS_CMSAF.tb.loc[ # Brightness Temps. for valid channels\n",
    "                time_i,DS_CMSAF.scene_channel.values[\n",
    "                    valid_channels],acrossT].values[0:7]  # TODO: [0:7] is hardcoded, not good\n",
    "\n",
    "            zenithAngle = DS_CMSAF.eia.loc[\n",
    "                time_i,acrossT].values.item() # expected to be a scalar; TODO add check\n",
    "            \n",
    "            print(\"%.2f s , TimeGetVars\" % (time.time()-startTimeGetVars))\n",
    "\n",
    "            \n",
    "            startTimeLoadAt1 = time.time()    \n",
    "            \n",
    "            # Load Atlases: (if any)    \n",
    "            ssmiRttov.SurfEmisRefl = np.zeros((\n",
    "                4, nprofiles, len(channels_list)), dtype=np.float64)\n",
    "            \n",
    "            print(\"%.2f s , TimeLoadAt1\" % (time.time()-startTimeLoadAt1))\n",
    "            \n",
    "            y_obs = pd.Series(BT,\n",
    "                index=channels_list\n",
    "            )\n",
    "            #print(y_obs)\n",
    "\n",
    "            # Definition of the observation variables\n",
    "            #y_vars = channels_list\n",
    "            \n",
    "            \n",
    "            # TODO Sy needs to be loaded from somewhere according to the channels_list: ***********\n",
    "            \n",
    "            # Channels 1-7, 12-16, 23, 24 (values from Deblonde-English 2003) (sigma or std)\n",
    "            y_noise = pd.Series(\n",
    "                    [\n",
    "                        2.4, 1.27, 1.44, 3.0, 1.34, 0.46, 0.47\n",
    "                    ],\n",
    "                    index=channels_list\n",
    "            )\n",
    "                            \n",
    "            # Variance values > std**2\n",
    "            S_y = pd.DataFrame(\n",
    "                np.diag(y_noise.values**2),\n",
    "                index=channels_list,\n",
    "                columns=channels_list,\n",
    "            )\n",
    "            # TODO Sy needs to be loaded from somewhere according to the channels_list  ***********\n",
    "            \n",
    "            datetime_obs = time_i.values\n",
    "            salinity = 35\n",
    "\n",
    "            startTimeGetAprio = time.time() \n",
    "            \n",
    "            band = get_band(lat,dir_bands,lat_bands) # gets the directory for the band where 'lat' is located\n",
    "            xa,Sa,xb,Sb = get_mean_covs(band, season) # gets mean and covariances for the band and season\n",
    "            \n",
    "            # reverse order in x and S (profile variables are reordered from low pressure to high) \n",
    "            Sb = Sb.iloc[::-1,::-1]\n",
    "            Sa = Sa.iloc[::-1,::-1]\n",
    "            xb = xb.iloc[::-1].squeeze() # convert means from dataframe to series\n",
    "            xa = xa.iloc[::-1].squeeze()\n",
    "            \n",
    "            print(\"%.2f s , TimeAprio\" % (time.time()-startTimeGetAprio))\n",
    "            \n",
    "            x_vars = xa.index.values\n",
    "            b_vars = xb.index.values\n",
    "            \n",
    "            # Get the pressure per level ; TODO: this needs to be independent of the variable (e.g. xb in this case)\n",
    "            xb_index = [float(i.split('_')[0]) for i in xb.index if i.endswith('temp')]\n",
    "            Pressure = np.array(xb_index).reshape(len(xb_index),1)\n",
    "                        \n",
    "            nlev = len([i for i in xb.index if i.endswith('temp')]) # number of levels in profile quantities\n",
    "\n",
    "\n",
    "            # forward_b_init fills \"myProfiles\" with the \"fixed\" parameters for the RTTOV simulation.\n",
    "            # The forward model F(x,b), RTTOV in our case, has two \"parameters\": x and b\n",
    "            # x is the state vector that is being retrieved (as such it is allowed to change during the retrieval)\n",
    "            # b contains all other parameters that are fixed during the retrieval (everything else that is not being retrieved)\n",
    "\n",
    "            startTimeForwbInit = time.time()\n",
    "            \n",
    "            forward_b_init(Pressure, salinity, \n",
    "                 lat, lon, datetime_obs, zenithAngle, myProfiles)\n",
    "            \n",
    "            print(\"%.2f s , TimeForwbInit\" % (time.time()-startTimeForwbInit))\n",
    "            \n",
    "            # Initialize multiple profiles for using a single call to RTTOV\n",
    "            # This is to be passed to the Jacobian function inside pyOpEst:\n",
    "            # The Jacobian is needed per parameter (len(xa.index)+len(xb.index))\n",
    "\n",
    "            nProfilesM = len(xa.index)+len(xb.index) # total number of parameters (x and b)\n",
    "            \n",
    "            \n",
    "            startTimeLoadAt2 = time.time()\n",
    "            \n",
    "            # Load Atlases for Multiple profiles: (if any)    \n",
    "            ssmiRttovM.SurfEmisRefl = np.zeros((\n",
    "                4, nProfilesM, len(channels_list)), dtype=np.float64) # RTTOVv12 used (2,nprof,nchan)\n",
    "            \n",
    "            print(\"%.2f s , TimeLoadAt2\" % (time.time()-startTimeLoadAt2))\n",
    "            \n",
    "            \n",
    "            \n",
    "            myProfilesM = pyrttov.Profiles(nProfilesM, nlev) # \n",
    "\n",
    "            press2 = np.ones((nlev, nProfilesM))*Pressure # Pressure:(nlev,1)\n",
    "\n",
    "        \n",
    "            startTimeForwbInitM = time.time()\n",
    "            \n",
    "            # Initialize profile datastructure for use in Jacobian computation:        \n",
    "            forward_b_init(press2, salinity, \n",
    "                  lat, lon, datetime_obs, zenithAngle, myProfilesM) \n",
    "            \n",
    "            print(\"%.2f s , TimeForwbInitM\" % (time.time()-startTimeForwbInitM))\n",
    "            \n",
    "            \n",
    "            # Define dictionary of parameters for the forward model:\n",
    "\n",
    "            forwardKwArgs = {\"myProfiles_a\" : myProfiles, \n",
    "                            \"ssmiRttov_a\" : ssmiRttov,\n",
    "                            \"channels_list\":channels_list.tolist()}   \n",
    "            \n",
    "            forwardKwArgsM = {\"myProfiles_a\" : myProfilesM, \n",
    "                             \"ssmiRttov_a\" : ssmiRttovM,\n",
    "                             \"channels_list\":channels_list.tolist()}\n",
    "            \n",
    "            \n",
    "            print(\"%.2f s , TimeRest\" % (time.time()-startTimeRest))\n",
    "            \n",
    "            oe_ref = pyOE.optimalEstimation( # oe_1 if windDisambiguation used\n",
    "                x_vars, # state variable names\n",
    "                xa,  # a priori\n",
    "                Sa, # a priori uncertainty\n",
    "                channels_list,  # measurement variable names\n",
    "                y_obs, # observations\n",
    "                S_y, # observation uncertainty\n",
    "                forwardRT, # forward Operator\n",
    "                forwardKwArgs=forwardKwArgs, # additonal function arguments\n",
    "                forwardKwArgsM=forwardKwArgsM, # additonal function arguments for jacobian \n",
    "                #x_truth=x_truth, # true profile\n",
    "                b_vars=b_vars,   # Parameter vector variable names\n",
    "                b_p=xb,        # Parameter vector \n",
    "                S_b=Sb        # Parameters error covariance matrix \n",
    "            )\n",
    "\n",
    "            count +=1\n",
    "            print(str(count)+' out of '+str(total_count))\n",
    "            \n",
    "            oe_ref.doRetrieval()\n",
    "            \n",
    "            if not oe_ref.converged :  # \n",
    "                DS_wind['wind'].loc[time_i,acrossT] = np.nan\n",
    "                DS_wind['wind_err'].loc[time_i,acrossT] = np.nan\n",
    "                print(y_obs)\n",
    "                print(band)\n",
    "                print(lat)\n",
    "                print(lon)\n",
    "                #ssmiRttov.SurfEmisRefl = None\n",
    "                continue            \n",
    "            \n",
    "            _, _, u10m, v10m, _, tsk,_ = supporting_routines_m.splitX_all_2(oe_ref.x_op)\n",
    "            _, _, u10m_err, v10m_err, _, tsk_err,_ = supporting_routines_m.splitX_all_2(oe_ref.x_op_err)\n",
    "            \n",
    "            w10m, w10m_err = supporting_routines_m.UV2Wvar(oe_ref.S_op.loc['00000_u10','00000_u10'],\n",
    "                                                  oe_ref.S_op.loc['00000_v10','00000_v10'],\n",
    "                                                  oe_ref.S_op.loc['00000_u10','00000_v10'],\n",
    "                                                  u10m.values.item(), v10m.values.item())\n",
    "\n",
    "            \n",
    "            DS_wind['wind'].loc[time_i,acrossT] =  w10m\n",
    "            DS_wind['wind_err'].loc[time_i,acrossT] =  w10m_err\n",
    "            \n",
    "            #ssmiRttov.SurfEmisRefl = None\n",
    "            \n",
    "            #print(DS_wind['wind'].loc[time,acrossT].values.item())\n",
    "            #print(DS_wind['wind_err'].loc[time,acrossT].values.item())\n",
    "            \n",
    "            \n",
    "            #gc.collect()\n",
    "            #sleep_and_print_mem(\"After iteration %d\" % i)\n",
    "            \n",
    "print(\"%.2f s , TimeAll\" % (time.time()-startTimeAll))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS_wind.chunk(chunks={'time':50}).to_netcdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.diagnostics import ProgressBar\n",
    "\n",
    "# or distributed.progress when using the distributed scheduler\n",
    "delayed_obj = DS_wind.chunk(chunks={'time':10}).to_netcdf(\"DS_wind.nc\", compute=False)\n",
    "\n",
    "with ProgressBar():\n",
    "     results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapPlotScat(x,y,data,namefile, orthoCenter=None):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    #ortho = ccrs.Orthographic(60,-15) #PlateCarree\n",
    "    ortho = ccrs.PlateCarree()\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    #crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.PlateCarree() #ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER  \n",
    "    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        vmin=1, vmax=2,  # 3,18\n",
    "        #vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"10m Wind Speed, RadEst [m/s]\")\n",
    "    #fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()\n",
    "\n",
    "swath_radEst = SwathDefinition(lons=DS_wind.lon.values, lats=DS_wind.lat.values)\n",
    "lons_radEst, lats_radEst = swath_radEst.get_lonlats()\n",
    "\n",
    "world_lons, world_lats, world_wind_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_wind.wind.values,\n",
    "                            radius_of_influence=3000)\n",
    "world_lons, world_lats, world_wind_err_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_wind.wind_err.values,\n",
    "                            radius_of_influence=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_radEst,\n",
    "                 'World_wind_RadEst_Plat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_err_radEst,\n",
    "                 'World_wind_Err_RadEst_Plat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_TB_frame(BT_scene_env1, area_interest, begin_t, end_t):\n",
    "    \n",
    "    lat_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lat[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    lon_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lon[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    tb_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.tb[begin_t:end_t,:,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "\n",
    "    grid_lons_interest, grid_lats_interest = area_interest.get_lonlats()\n",
    "\n",
    "    swath_scene1 = SwathDefinition(lons=lon_scene1, lats=lat_scene1)\n",
    "    lons_scene1, lats_scene1 = swath_scene1.get_lonlats()\n",
    "\n",
    "    reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "\n",
    "    return reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1\n",
    "    #swath_reduced_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def defineArea(corners, proj_id, datum):\n",
    "    #corners=parseMeta(data_name)\n",
    "\n",
    "    lat_0 = '{lat_0:5.2f}'.format_map(corners)\n",
    "    lon_0= '{lon_0:5.2f}'.format_map(corners)\n",
    "    lon_bbox = [corners['min_lon'],corners['max_lon']]\n",
    "    lat_bbox = [corners['min_lat'],corners['max_lat']]\n",
    "#    area_dict = dict(datum=datum,lat_0=lat_0,lon_0=lon_0,\n",
    "#                proj=proj_id,units='m')\n",
    "\n",
    "    area_dict = dict(datum=datum,lat_0=-15,lon_0=60,\n",
    "                proj=proj_id,units='m',a=6370997.0,)\n",
    "\n",
    "    prj=pyproj.Proj(area_dict)\n",
    "    x, y = prj(lon_bbox, lat_bbox)\n",
    "    xsize=200\n",
    "    ysize=200\n",
    "    area_id = 'granule'\n",
    "    area_name = 'modis swath 5min granule'\n",
    "    area_extent = (x[0], y[0], x[1], y[1])\n",
    "    print(area_extent)\n",
    "    area_def = AreaDefinition(area_id, area_name, proj_id, \n",
    "                                   area_dict, xsize, ysize,area_extent)\n",
    "    return area_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation of area of interest:\n",
    "#corners = {\"min_lon\": 25 , \"max_lon\": 75, \"min_lat\": -30 , \"max_lat\": 0, \"lat_0\": 60, \"lon_0\":-15}\n",
    "corners = {\"min_lon\": -95 , \"max_lon\": 20, \"min_lat\": 3 , \"max_lat\": 50, \"lat_0\": 27, \"lon_0\":-57}\n",
    "proj_id = 'eqc'  # eqc\n",
    "datum = 'WGS84'\n",
    "area_interest = defineArea(corners, proj_id, datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeMapAnimScat(BT_scene, BT_attributes, channel, area, \n",
    "                      init_date, nFrames, delta_hours, namefile):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(frameon=False) #figsize=(8, 6))\n",
    "    fig.add_axes([0,0,1,1])\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    ax.set_title(\"TB \"+namefile)\n",
    "    #ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black') \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "\n",
    "\n",
    "    #delta_hours = 12\n",
    "    end_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "    time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "        BT_attributes.time.values<end_date))\n",
    "    begin_t = time_slice[0][0]  \n",
    "    end_t = time_slice[0][-1]\n",
    "\n",
    "    x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "    \n",
    "    ims = []\n",
    "    im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12     # 180, 270\n",
    "            vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    \n",
    "    #fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    \n",
    "    for i in np.arange(nFrames):\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12  # 180, 270\n",
    "            vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "        \n",
    "        ims.append([im1])\n",
    "        init_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "        end_date = end_date + np.timedelta64(delta_hours, 'h') \n",
    "        time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "            BT_attributes.time.values<end_date))\n",
    "        begin_t = time_slice[0][0]  \n",
    "        end_t = time_slice[0][-1] \n",
    "        x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    plt.tight_layout()\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 12\n",
    "delta_hours = 12\n",
    "channel = 2\n",
    "namefile = 'env1_22V_12h_F16'\n",
    "timeMapAnimScat(BT_scene, BT_attributes, channel, area_interest, \n",
    "                      init_date, nFrames, delta_hours, namefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath scene 1 at a world wide scale:\n",
    "\n",
    "result_scene1 = resample_nearest(swath_scene1, tb_scene1.values, area_def_world, \n",
    "                          radius_of_influence=30000, fill_value=np.nan)\n",
    "\n",
    "#result_scene2 = resample_nearest(swath_scene2, tb_scene2.values, area_def_world, \n",
    "#                          radius_of_influence=30000, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "\n",
    "\n",
    "#reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2 = \\\n",
    "#                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "#                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "#                            radius_of_influence=3000)\n",
    "#swath_reduced_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "world_lons_scene1, world_lats_scene1, world_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)\n",
    "\n",
    "world_lons_scene2, world_lats_scene2, world_data_scene2 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath data into a grid in the area of interest \n",
    "result_reduced_scene1 = resample_nearest(swath_reduced_scene1, reduced_data_scene1, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)\n",
    "\n",
    "result_reduced_scene2 = resample_nearest(swath_reduced_scene2, reduced_data_scene2, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapArea('mapArea0', area_def_world)\n",
    "mapArea('mapArea01', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chann = 0\n",
    "# Plot resampled (grid version) scenes:\n",
    "basicMapPlot(result_scene1[:,:,chann],'scene1'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene1[:,:,chann],\n",
    "             'scene1_reduced'+str(chann), area_interest)  # map only the area of interest, grid\n",
    "\n",
    "basicMapPlot(result_scene2[:,:,chann],'scene2'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene2[:,:,chann],\n",
    "             'scene2_reduced'+str(chann), area_interest)  # map only the area of interest, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in area of interest, Plate Carree projection\n",
    "\n",
    "chann = 0\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19H', area_interest, )\n",
    "chann = 1\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19V', area_interest, )\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_22V', area_interest, )\n",
    "#chann = 1\n",
    "#basicMapPlotScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene2_scatt_PCarr_91V', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path('/nobackup/users/echeverr/fortran_tests/netcdf/following_Edouard/imgs/ch19H')\n",
    "images = list(image_path.glob('*.png'))\n",
    "image_list = []\n",
    "for file_name in images:\n",
    "    image_list.append(imageio.imread(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimwrite('animated_from_images.gif', image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in world area, Orthographic projection\n",
    "\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat1(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'world_scene1_scatt_Orth_22V', area_interest)\n",
    "#chann = 3\n",
    "#basicMapPlotScat1(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'world_scene1_scatt_Orth_91H', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot original swath pixels:\n",
    "chann = 2\n",
    "basicMapPlotScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'scene1_scatt_Orth_world_22V', area_def_world)\n",
    "chann = 3\n",
    "basicMapPlotScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'scene1_scatt_Orth_world_91H', area_def_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlot(result,namefile, area):\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    #ax.background_img(name='BM', resolution='high') \n",
    "    ax.coastlines();\n",
    "    #ax.stock_img();\n",
    "    ax.grid(True)\n",
    "    #ax.set_xlabel('Longitude [deg]')\n",
    "    #ax.set_ylabel('Latitude [deg]')\n",
    "\n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    #gl.xlabels_top = False\n",
    "    #gl.ylabels_left = False\n",
    "    #gl.ylabels_right=True\n",
    "    #gl.xlines = True\n",
    "    #gl.xlocator = mticker.FixedLocator([70, 75, 80, 85])\n",
    "    #gl.ylocator = mticker.FixedLocator([-5, -3, -1, 1, 3])\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    im = ax.imshow(result, transform=crs, extent=crs.bounds, origin='upper', cmap='jet', vmin=150, vmax=250)\n",
    "    fig.colorbar(im,ax=ax) \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)   \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "    ax.set_title(\"TB\")\n",
    "    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        #vmin=3, vmax=18         #180, 270\n",
    "        vmin=130, vmax=270         #180, 270\n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"Brightness temperature [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapArea(namefile, area):\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    \n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "    \n",
    "    ax.coastlines(linewidth=0.5)   \n",
    "    #ax.set_global() \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chann = 0\n",
    "#basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene1_scatt'+str(chann), area_interest)\n",
    "\n",
    "nFrames = 150\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    #ax.set_title(\"TB \"+namefile)\n",
    "    ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "   \n",
    "    #gl0 = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl0.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl0.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = x[start_:end_]\n",
    "    y2 = y[start_:end_]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12     # 180, 270\n",
    "            #vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12  # 180, 270\n",
    "            #vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,x[start_:end_])\n",
    "        y2 = np.append(y2,y[start_:end_])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'_bar.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BT_scene_env2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat1(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ortho = ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    \n",
    "    #ax.set_title(\"TB\")\n",
    "    #ax.coastlines() \n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        #vmin=3, vmax=18,  # 180, 270\n",
    "        vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    #fig.colorbar(im).set_label(\"10m Wind Speed, HOAPS [m/s]\")\n",
    "    fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 1200\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat1(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    ortho = ccrs.Orthographic(-39,18) #ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "       \n",
    "    ax.set_title(\"Wind Speed \"+namefile) \n",
    "    #ax.set_title(\"Temperature Brightness \"+namefile) \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = xy[start_:end_,0]\n",
    "    y2 = xy[start_:end_,1]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18  # 180, 250\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18   # 180, 270 TB\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,xy[start_:end_,0])\n",
    "        y2 = np.append(y2,xy[start_:end_,1])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=30, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
