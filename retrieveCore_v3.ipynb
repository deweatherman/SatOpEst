{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import ftplib\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime \n",
    "import time\n",
    "from datetime import datetime \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import scipy.stats as stats\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "##import sklearn\n",
    "#from sklearn.metrics import median_absolute_error, mean_squared_error,r2_score\n",
    "#from sklearn.linear_model import LinearRegression, RANSACRegressor, HuberRegressor\n",
    "\n",
    "\n",
    "import pyOptimalEstimation as pyOE\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cmocean import cm as cmo\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from glob import glob\n",
    "\n",
    "#import imageio\n",
    "\n",
    "#from pathlib import Path\n",
    "\n",
    "#from netCDF4 import Dataset\n",
    "\n",
    "import gc, psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing\n",
    "#multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sys.path.append('/nobackup/users/echeverr/Git/SatOpEst/support') # where supporting_routines_m live\n",
    "#sys.path.append('/home/mario/Documents/work/code/git/SatOpEst/support') # where supporting_routines_m live\n",
    "sys.path.append('/usr/people/echeverr/Data_m/support') # where supporting_routines_m live\n",
    "\n",
    "import supporting_routines_m \n",
    "\n",
    "import os\n",
    "\n",
    "rttov_installdir = '/usr/people/echeverr/Documents/code/nwpsaf/rttov13'\n",
    "#rttov_installdir = '/home/mario/myLibs/rrtov13/rttov130'\n",
    "\n",
    "sys.path.append(rttov_installdir+'/wrapper')\n",
    "import pyrttov\n",
    "\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "#os.environ[\"CARTOPY_USER_BACKGROUNDS\"] = os.path.join(current_directory,'/nobackup/users/echeverr/py_tests/earthpy_example/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = psutil.Process()\n",
    "\n",
    "\n",
    "\n",
    "#BT_dir = '/nobackup/users/echeverr/data/cmsaf/ssmis/F16/'\n",
    "#BT_dir = '/home/mario/Data/CMSAF/ssims/F16/'\n",
    "BT_dir = '/usr/people/echeverr/Data_m/TB_F16/'\n",
    "\n",
    "BT_dir2 = BT_dir+'V1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BT_file = '*.nc'\n",
    "BT_file = 'BTRin20140909000000324SSF1601GL'\n",
    "\n",
    "BT_file2 = BT_file+'_v1'\n",
    "\n",
    "# Chunking the dataset (for 30 mins 10,10 works well (in my laptop, Mario); \n",
    "# if increase minutes times x, then increase chunk_size_time times x as well (avoids memory problems))\n",
    "chunk_size_time =  10 # 420 for half day\n",
    "chunk_size_s_a_t = 10\n",
    "\n",
    "# user input:\n",
    "#init_date = np.datetime64('2014-09-09T00:25:00.000') \n",
    "#end_date = np.datetime64('2014-09-09T00:27:00.000')\n",
    "init_date = np.datetime64('2014-09-09T00:00:00.000') \n",
    "end_date = np.datetime64('2014-09-09T01:59:59.000')\n",
    "#init_date = np.datetime64('2014-09-09T00:00:00.000') \n",
    "#end_date = np.datetime64('2014-09-09T23:59:59.000')\n",
    "\n",
    "# Perform chisquare tests available in pyOptimalEstimation or not:\n",
    "performTests = True\n",
    "\n",
    "#Latitude bands:\n",
    "#lat_bands2 = np.array([-90.0,-40.0,-40.0,0.0,0.0,40.0,40.0,90.0])\n",
    "\n",
    "# Directories for lat_bands:\n",
    "\n",
    "#aprioLowCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lower_cap/'\n",
    "#aprioLowMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lowmid_cap/'\n",
    "#aprioUpMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upmid_cap/'\n",
    "#aprioUpCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upper_cap/'\n",
    "\n",
    "#aprioLowCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lower_cap/'\n",
    "#aprioLowMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lowmid_cap/'\n",
    "#aprioUpMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upmid_cap/'\n",
    "#aprioUpCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upper_cap/'\n",
    "\n",
    "#dir_bands2 = np.array([aprioLowCapDir,aprioLowMidCapDir,aprioUpMidCapDir,aprioUpCapDir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xr.open_dataset(BT_dir2+BT_file2+'5chan'+'.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "import pyresample\n",
    "from pyresample import create_area_def, load_area, data_reduce, utils, AreaDefinition\n",
    "from pyresample.geometry import SwathDefinition\n",
    "from pyresample.kd_tree import resample_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the directory (str) corrisponding to the band where \"lat\" is located\n",
    "def get_band(lat,dir_bands,lat_bands):\n",
    "    i=0\n",
    "    band_out=None\n",
    "    for band in dir_bands: \n",
    "        if(np.logical_and(lat >= lat_bands[i][0], lat <= lat_bands[i][1])):\n",
    "            band_out = band\n",
    "        i+=1\n",
    "    if(band_out==None):\n",
    "        print(\"Error in lat:\")\n",
    "        print(lat)\n",
    "        print(type(lat))\n",
    "        sys.stderr.write(\"Error in lat\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return band_out\n",
    "\n",
    "\n",
    "def split_file_name(f):\n",
    "    aux1 = f.split(\".\")[0].split(\"0_\")\n",
    "    aux2 = aux1[1].split(\"_\")\n",
    "    season = aux1[0]\n",
    "    aORb = aux2[0]\n",
    "    meanORcov = aux2[1]\n",
    "    return season, aORb, meanORcov\n",
    "\n",
    "def get_mean_covs(directory,inputSeason):\n",
    "# very badly programed look-up table: it will look\n",
    "# for the files corrisponding to a given inputSeason \n",
    "# and return means and covariances for both state vector (a)\n",
    "# and parameters vector (b).\n",
    "    files = os.listdir(directory)\n",
    "    for f in files:\n",
    "        #print(f)\n",
    "        season, aORb, meanORcov = split_file_name(f)\n",
    "        if(inputSeason==season):\n",
    "            if(aORb=='a'):\n",
    "                if(meanORcov=='mean'):\n",
    "                    xa = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                elif(meanORcov=='cov'):   \n",
    "                    Sa = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                else:\n",
    "                    print('Something went wrong, check:')\n",
    "                    print(directory+f)\n",
    "            elif(aORb=='b'):\n",
    "                if(meanORcov=='mean'):\n",
    "                    xb = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                elif(meanORcov=='cov'):   \n",
    "                    Sb = pd.read_csv(directory+f, \n",
    "                                     index_col=0).rename_axis('state', \n",
    "                                                              axis=0).rename_axis('stateT', \n",
    "                                                                                  axis=1)\n",
    "                else:\n",
    "                    print('Something went wrong, check:')\n",
    "                    print(directory+f)                \n",
    "            \n",
    "        \n",
    "    return xa,Sa,xb,Sb   \n",
    "\n",
    "get_mean_covs_vec = np.vectorize(get_mean_covs, otypes=[pd.core.frame.DataFrame,\n",
    "                                                        pd.core.frame.DataFrame,\n",
    "                                                        pd.core.frame.DataFrame,\n",
    "                                                        pd.core.frame.DataFrame])\n",
    "\n",
    "def reshape4profiles(profiles):\n",
    "    # \"profiles\" is a numpy array\n",
    "    # \"profiles\" can contain 1 or more profiles\n",
    "    # \"profiles\" has dimensions (nlevels, nprofiles)\n",
    "    # \"outProfiles\" has dimensions (nprofiles,nlevels) (as needed in RTTOV)\n",
    "    \n",
    "    if (len(profiles.shape)==1):\n",
    "        outProfiles = profiles.reshape(1,profiles.shape[0]).copy()\n",
    "    else:\n",
    "        outProfiles = profiles.T.copy() #profiles.reshape(profiles.shape[1]\n",
    "            #                          ,profiles.shape[0]) \n",
    "    return outProfiles  \n",
    "\n",
    "def expand2nprofiles(n, nprof):\n",
    "    # Transform 1D array to a [nprof, nlevels] array\n",
    "    outp = np.empty((nprof, len(n)), dtype=n.dtype)\n",
    "    for i in range(nprof):\n",
    "        outp[i, :] = n[:]\n",
    "    return outp\n",
    "\n",
    "def forward_b_init(pressure, salinity, lat, long, datetime_obs64, \n",
    "                   zenithAngle, myProfiles):\n",
    "    \n",
    "    if (len(pressure.shape)==1):\n",
    "        nprofiles = 1\n",
    "    else:\n",
    "        nprofiles =  pressure.shape[1]\n",
    "    \n",
    "    # The rest of the code uses datetime64 format (numpy), but I have to pass the obs date as integers to RTTOV\n",
    "    datetime_obs = supporting_routines_m.datetime64_to_datetime(datetime_obs64)\n",
    "    \n",
    "    s2m = np.zeros((nprofiles,6), dtype=np.float64) # s2m has 6 elements (docs RTTOV)\n",
    "    \n",
    "    angles = np.zeros((nprofiles,4), dtype=np.float64) # angles has 4 elements (docs RTTOV)\n",
    "    angles[:,0] = zenithAngle\n",
    "    \n",
    "    \n",
    "    # for RTTOV 13 skin is 9 elements long:\n",
    "    skin = np.zeros((nprofiles,9), dtype=np.float64) # skin has 9 elements (docs RTTOV)\n",
    "    skin[:,1] = salinity\n",
    "        \n",
    "    surftype = np.zeros((nprofiles,2), dtype=np.int32) # surftype has 2 elements (docs RTTOV)\n",
    "    surftype[:,:] = 1 # [sea, ocean] Harcoded for now, TODO *** mario\n",
    "    \n",
    "    \n",
    "    surfgeom = np.zeros((nprofiles,3), dtype=np.float64) # surfgeom has 3 elements (docs RTTOV)\n",
    "    surfgeom[:,0] = lat\n",
    "    surfgeom[:,1] = long\n",
    "    # surfgeom[:,2]=0 # elevation harcoded to 0 for now, TODO *** mario\n",
    "    \n",
    "    date_times = np.zeros((nprofiles,6), dtype=np.int32) # date_times has 6 elements (docs RTTOV)\n",
    "    date_times[:,0] = datetime_obs.year\n",
    "    date_times[:,1] = datetime_obs.month\n",
    "    date_times[:,2] = datetime_obs.day\n",
    "    date_times[:,3] = datetime_obs.hour\n",
    "    date_times[:,4] = datetime_obs.minute\n",
    "    date_times[:,5] = datetime_obs.second\n",
    "    \n",
    "    \n",
    "    myProfiles.GasUnits = 1  # kg/kg (see RTTOV doc. for other options) # Harcoded for now, TODO *** mario\n",
    "    myProfiles.P = reshape4profiles(pressure) \n",
    "    myProfiles.S2m = s2m\n",
    "    myProfiles.Angles = angles\n",
    "    myProfiles.Skin = skin\n",
    "    myProfiles.SurfType = surftype\n",
    "    myProfiles.SurfGeom = surfgeom\n",
    "    myProfiles.DateTimes = date_times \n",
    "\n",
    "\n",
    "def forwardRT(X, myProfiles_a, ssmiRttov_a, channels_list=None):\n",
    "    \n",
    "    # TODO: Add assertions, tests *** mario\n",
    "\n",
    "    # X contains T, Q and W10, lets split the vector\n",
    "    #temperature, humidity, wind10m = supporting_routines_m.splitX(X)\n",
    "    \n",
    "    # if wind speed in components:\n",
    "\n",
    "    #temperature, humidity, u10m, v10m, bp2m, bt2m, btsk \\\n",
    "    #= supporting_routines_m.splitX_all(X)\n",
    "    \n",
    "    #NEW\n",
    "    # Wind u, v:\n",
    "    #temperature, humidity, u10m, v10m, t2m, tsk, sp \\\n",
    "    #= supporting_routines_m.splitX_all_2(X)\n",
    "    # Wind W:\n",
    "    temperature, humidity, w10m, t2m, tsk, sp \\\n",
    "    = supporting_routines_m.splitX_all_2W(X)    \n",
    "    \n",
    "    # humdity is in log10 scale, convert to linear in kg/kg\n",
    "    humidity = (10**humidity) / 1000.\n",
    "    # or abs_humidity? *** note mario\n",
    "\n",
    "    myProfiles_a.T = reshape4profiles(temperature.to_numpy(dtype=np.float64))  \n",
    "    myProfiles_a.Q = reshape4profiles(humidity.to_numpy(dtype=np.float64))  \n",
    "\n",
    "    myProfiles_a.S2m[:,0] = reshape4profiles(\n",
    "        sp.to_numpy(dtype=np.float64)).flatten() # surface pressure\n",
    "    myProfiles_a.S2m[:,1] = reshape4profiles(\n",
    "        t2m.to_numpy(dtype=np.float64)).flatten()  # 2m temperature\n",
    "    myProfiles_a.Skin[:,0] = reshape4profiles(\n",
    "        tsk.to_numpy(dtype=np.float64)).flatten() \n",
    "    \n",
    "    # if wind in components:\n",
    "    #myProfiles_a.S2m[:,3] = reshape4profiles(\n",
    "    #    u10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, u component\n",
    "    #myProfiles_a.S2m[:,4] = reshape4profiles(\n",
    "    #    v10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, v component \n",
    "    \n",
    "    # Wind W:\n",
    "    myProfiles_a.S2m[:,3] = 0.7071*reshape4profiles(\n",
    "        w10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed\n",
    "    myProfiles_a.S2m[:,4] = myProfiles_a.S2m[:,3]    \n",
    "   \n",
    "    ssmiRttov_a.Profiles = myProfiles_a\n",
    "    \n",
    "    #ssmiRttov_a.SurfEmisRefl[:,:,:] = -1. # need to \"reset\" to -1 every time RTTOV is called; \n",
    "    # -1 indicates to RTTOV to use internal values for surface emissivity.\n",
    "\n",
    "    try:\n",
    "        ssmiRttov_a.runDirect(channels_list)\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error running RTTOV direct model: {!s}\".format(e))\n",
    "        sys.exit(1)    \n",
    "        \n",
    "    #print(ssmiRttov.BtRefl[:, :].shape)\n",
    "    #print(ssmiRttov.BtRefl[:, :])\n",
    "    \n",
    "    if(ssmiRttov_a.BtRefl[:, :].shape[0]==1):\n",
    "        TB = ssmiRttov_a.BtRefl[0, :].T\n",
    "    else:\n",
    "        TB = ssmiRttov_a.BtRefl[:, :].T\n",
    "    \n",
    "    return TB\n",
    "\n",
    "def rttovK(X, perturbation, y_var, myProfiles_a, ssmiRttov_a, channels_list=None):\n",
    "    \n",
    "    # TODO: Add assertions, tests *** mario\n",
    "\n",
    "    #temperature, humidity, u10m, v10m, t2m, tsk, sp \\\n",
    "    #= supporting_routines_m.splitX_all_2(X)\n",
    "\n",
    "    # Wind W:\n",
    "    temperature, humidity, w10m, t2m, tsk, sp \\\n",
    "    = supporting_routines_m.splitX_all_2W(X)        \n",
    "    \n",
    "    # humdity is in log10 scale, convert to linear in kg/kg\n",
    "    humidity = (10**humidity) / 1000.\n",
    "\n",
    "    myProfiles_a.T = reshape4profiles(temperature.to_numpy(dtype=np.float64))  \n",
    "    myProfiles_a.Q = reshape4profiles(humidity.to_numpy(dtype=np.float64))  \n",
    "\n",
    "    myProfiles_a.S2m[:,0] = reshape4profiles(\n",
    "        sp.to_numpy(dtype=np.float64)).flatten() # surface pressure\n",
    "    myProfiles_a.S2m[:,1] = reshape4profiles(\n",
    "        t2m.to_numpy(dtype=np.float64)).flatten()  # 2m temperature\n",
    "    myProfiles_a.Skin[:,0] = reshape4profiles(\n",
    "        tsk.to_numpy(dtype=np.float64)).flatten() \n",
    "    \n",
    "    # if wind in components:\n",
    "    #myProfiles_a.S2m[:,3] = reshape4profiles(\n",
    "    #    u10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, u component\n",
    "    #myProfiles_a.S2m[:,4] = reshape4profiles(\n",
    "    #    v10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed, v component   \n",
    "    \n",
    "    # Wind W:\n",
    "    myProfiles_a.S2m[:,3] = 0.7071*reshape4profiles(\n",
    "        w10m.to_numpy(dtype=np.float64)).flatten()  #  10m windspeed\n",
    "    myProfiles_a.S2m[:,4] = myProfiles_a.S2m[:,3] \n",
    "    \n",
    "    ssmiRttov_a.Profiles = myProfiles_a\n",
    "    \n",
    "    #ssmiRttov_a.SurfEmisRefl[:,:,:] = -1. # need to \"reset\" to -1 every time RTTOV is called; \n",
    "    # -1 indicates to RTTOV to use internal values for surface emissivity.\n",
    "    \n",
    "    try:\n",
    "        ssmiRttov_a.runK(channels_list)\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error running RTTOV direct model: {!s}\".format(e))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create Jacobian Dataframe skeleton:\n",
    "    jacx = pd.DataFrame(index=y_var, columns=X.index)  \n",
    "    \n",
    "    # Fill-in Jacobian Dataframe\n",
    "    jacx = supporting_routines_m.mergeX_all_2W(jacx.T, X, \n",
    "                                            ssmiRttov_a,\n",
    "                                            perturbation = perturbation, \n",
    "                                            LogHum=True)\n",
    "\n",
    "    return jacx.T.to_numpy(dtype=np.float64) \n",
    "\n",
    "\n",
    "def sleep_and_print_mem(title, sleep=3):\n",
    "    time.sleep(sleep)\n",
    "    print(\"\\n\" + title + \" : \" +  \"%0.2f MB\" % (p.memory_info().rss / 1e6)\n",
    "         + \"   \" + \"=\" * 49)\n",
    "    \n",
    "def month2season(month):\n",
    "    seasons = [\n",
    "        'DJF',\n",
    "        'MAM',\n",
    "        'JJA',\n",
    "        'SON',\n",
    "    ]\n",
    "    season = seasons[month-1] # month is 1 based index while seasons array needs 0 based indexing\n",
    "   \n",
    "    return season\n",
    "\n",
    "month2season_vec = np.vectorize(month2season, otypes=[str])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rttovStrucInit():\n",
    "    # Set knobs for use of OpenMP in RTTOV\n",
    "\n",
    "    NthreadsF = 1  # Number of threads for forward model  #os.cpu_count()\n",
    "    NthreadsFM = 4 # Number of threads for forward model (in case of multiple profiles, used in) #os.cpu_count()\n",
    "    NprofsPerCallFM = 40 # ?Roughly speaking number of variables (e.g. 279) / Number of threads (e.g. 8)\n",
    "    # Create instance of Profiles class; \n",
    "    # it's a container of the input atmospheric state that RTTOV will simulate\n",
    "\n",
    "\n",
    "\n",
    "    #  Create instance of RTTOV\n",
    "    ssmiRttov = pyrttov.Rttov()\n",
    "\n",
    "    #chan_list_ssmi = (12,13,14,15,16,17,18) #(1,12,13,14,15,16) #  #\n",
    "\n",
    "    # Define instrument (FileCoef):\n",
    "    ssmiRttov.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "    # Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "    # so read all channels\n",
    "    try:\n",
    "        ssmiRttov.loadInst() #chan_list_ssmi\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Some settings\n",
    "    ssmiRttov.Options.AddInterp = True\n",
    "    # ssmiRttov.Options.InterpMode = 2\n",
    "    ssmiRttov.Options.CO2Data = False\n",
    "    ssmiRttov.Options.VerboseWrapper = False\n",
    "    ssmiRttov.Options.DoCheckinput = False\n",
    "    ssmiRttov.Options.UseQ2m = False\n",
    "    ssmiRttov.Options.ApplyRegLimits = True\n",
    "    ssmiRttov.Options.Verbose = False\n",
    "    ssmiRttov.Options.FastemVersion = 6\n",
    "    ssmiRttov.Options.Switchrad = True\n",
    "    ssmiRttov.Options.Nthreads = NthreadsF\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    return ssmiRttov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR MULTIPLE PROFILES CALL TESTING\n",
    "\n",
    "def rttovMStrucInit():\n",
    "    \n",
    "    NthreadsFM = 4 # Number of threads for forward model (in case of multiple profiles, used in) #os.cpu_count()\n",
    "    NprofsPerCallFM = 40 # ?Roughly speaking number of variables (e.g. 279) / Number of threads (e.g. 8)\n",
    "    ssmiRttovM = pyrttov.Rttov()\n",
    "\n",
    "    # SSMIS:\n",
    "    ssmiRttovM.FileCoef = '{}/{}'.format(rttov_installdir,\n",
    "                                    \"rtcoef_rttov13/rttov7pred54L/rtcoef_dmsp_16_ssmis.dat\")\n",
    "\n",
    "    ssmiRttovM.Options.AddInterp = True\n",
    "    #ssmiRttovM.Options.InterpMode = 2\n",
    "    ssmiRttovM.Options.CO2Data = False\n",
    "    ssmiRttovM.Options.VerboseWrapper = False\n",
    "    ssmiRttovM.Options.DoCheckinput = False\n",
    "    ssmiRttovM.Options.UseQ2m = False\n",
    "    ssmiRttovM.Options.ApplyRegLimits = True\n",
    "    ssmiRttovM.Options.Verbose = False\n",
    "    ssmiRttovM.Options.FastemVersion = 6 \n",
    "    ssmiRttovM.Options.Nthreads = NthreadsFM\n",
    "    #ssmiRttovM.Options.NprofsPerCall = NprofsPerCallFM\n",
    "    ssmiRttovM.Options.Switchrad = True\n",
    "\n",
    "    # Load the instruments: for HIRS and MHS do not supply a channel list and\n",
    "    # so read all channels\n",
    "    try:\n",
    "        ssmiRttovM.loadInst()\n",
    "    except pyrttov.RttovError as e:\n",
    "        sys.stderr.write(\"Error loading instrument(s): {!s}\".format(e))\n",
    "        sys.exit(1)\n",
    "    return ssmiRttovM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add init function:\n",
    "\n",
    "\n",
    "def init_retrieval_data(time_i, lat, lon, \n",
    "                        channels_list,\n",
    "                        channel_uncertainty, BT,\n",
    "                        zenithAngle, #dir_bands2,\n",
    "                        #lat_bands2\n",
    "                       ):  \n",
    "\n",
    "    nprofiles = 1  # This is hardcoded (we use RTTOV within OE, workin on a single profile per OE)\n",
    "    nlev = 37 # TODO: this needs to be read from the apriori data (covariances and means) \n",
    "    myProfiles = pyrttov.Profiles(nprofiles, nlev)\n",
    "    ssmiRttov = rttovStrucInit()  \n",
    "    ssmiRttovM=None  # rttovMStrucInit() \n",
    "\n",
    "    #aprioLowCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lower_cap/'\n",
    "    #aprioLowMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/lowmid_cap/'\n",
    "    #aprioUpMidCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upmid_cap/'\n",
    "    #aprioUpCapDir = '/home/mario/Data/Covariance_means/CDS_api_data/ERA5_data/upper_cap/'\n",
    "    \n",
    "    #aprioLowCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lower_cap/'\n",
    "    #aprioLowMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/lowmid_cap/'\n",
    "    #aprioUpMidCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upmid_cap/'\n",
    "    #aprioUpCapDir = '/nobackup/users/echeverr/data/ECMWF_era5/hourly_ERA5/mean_cov/upper_cap/'\n",
    "    \n",
    "    aprioLowCapDir = '/usr/people/echeverr/Data_m/mean_cov/lower_cap/'\n",
    "    aprioLowMidCapDir = '/usr/people/echeverr/Data_m/mean_cov/lowmid_cap/'\n",
    "    aprioUpMidCapDir = '/usr/people/echeverr/Data_m/mean_cov/upmid_cap/'\n",
    "    aprioUpCapDir = '/usr/people/echeverr/Data_m/mean_cov/upper_cap/'\n",
    "    \n",
    "\n",
    "    # list of directories contaning covariances and means; each dir contains one geographical zone \n",
    "    # Geo. zones are divided in latitude strips: [-90,-40), [-40,0), [0,+40), [+40,+90]; lon. [-180,+180) for all\n",
    "    \n",
    "    dir_bands = [aprioLowCapDir,aprioLowMidCapDir,aprioUpMidCapDir,aprioUpCapDir] # \n",
    "    lat_bands = [[-90.0,-40.0],[-40.0,0.0],[0.0,40.0],[40.0,90.0]]   # \n",
    "\n",
    "    #dir_bands = dir_bands2.reshape((4,)).tolist()\n",
    "    #lat_bands = lat_bands2.reshape((4,2)).tolist()\n",
    "    \n",
    "    months = (time_i.astype('datetime64[M]').astype(int) % 12 + 1)%12 // 3 + 1\n",
    "    season = month2season_vec(months)\n",
    "   \n",
    "    \n",
    "    # Load Atlases: (if any)    \n",
    "    #ssmiRttov.SurfEmisRefl = np.zeros((\n",
    "    #    4, nprofiles, len(channels_list)), dtype=np.float64)\n",
    "            \n",
    "    y_obs = pd.Series(BT,\n",
    "                index=channels_list\n",
    "                     )            \n",
    "            \n",
    "    # TODO Sy needs to be loaded from somewhere according to the channels_list: ***********\n",
    "            \n",
    "    # Channels 12-16 (values from Deblonde-English 2003) (sigma or std)\n",
    "    y_noise = pd.Series(channel_uncertainty,\n",
    "                        index=channels_list)\n",
    "                            \n",
    "    # Variance values > std**2\n",
    "    S_y = pd.DataFrame(\n",
    "                np.diag(y_noise.values**2),\n",
    "                index=channels_list,\n",
    "                columns=channels_list,\n",
    "    )\n",
    "    # TODO Sy needs to be loaded from somewhere according to the channels_list  ***********\n",
    "            \n",
    "    datetime_obs = time_i #.values\n",
    "    salinity = 35 # hardcoded, not good; TODO Mario\n",
    "            \n",
    "    band = get_band(lat,dir_bands,lat_bands) # gets the directory for the band where 'lat' is located\n",
    "    xa,Sa,xb,Sb = get_mean_covs(band, season) # gets mean and covariances for the band and season\n",
    "            \n",
    "    # reverse order in x and S (profile variables are reordered from low pressure to high) \n",
    "    Sb = Sb.iloc[::-1,::-1]\n",
    "    Sa = Sa.iloc[::-1,::-1]\n",
    "    xb = xb.iloc[::-1].squeeze() # convert means from dataframe to series\n",
    "    xa = xa.iloc[::-1].squeeze()\n",
    "            \n",
    "    x_vars = xa.index.values\n",
    "    b_vars = xb.index.values\n",
    "            \n",
    "    # Get the pressure per level ; TODO: this needs to be independent of the variable (e.g. xb in this case)\n",
    "    xb_index = [float(i.split('_')[0]) for i in xb.index if i.endswith('temp')]\n",
    "    Pressure = np.array(xb_index).reshape(len(xb_index),1)\n",
    "                        \n",
    "    nlev = len([i for i in xb.index if i.endswith('temp')]) # number of levels in profile quantities\n",
    "\n",
    "\n",
    "    # forward_b_init fills \"myProfiles\" with the \"fixed\" parameters for the RTTOV simulation.\n",
    "    # The forward model F(x,b), RTTOV in our case, has two \"parameters\": x and b\n",
    "    # x is the state vector that is being retrieved (as such it is allowed to change during the retrieval)\n",
    "    # b contains all other parameters that are fixed during the retrieval (everything else that is not being retrieved)\n",
    "\n",
    "    forward_b_init(Pressure, salinity,\n",
    "                   lat, lon, datetime_obs, zenithAngle, myProfiles)\n",
    "    \n",
    "    # Define dictionary of parameters for the forward model:\n",
    "\n",
    "    forwardKwArgs = {\"myProfiles_a\" : myProfiles, \n",
    "                    \"ssmiRttov_a\" : ssmiRttov,\n",
    "                    \"channels_list\":channels_list.tolist()}    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Define dictionary of parameters for the forward model (Multiple profiles case):\n",
    "    \n",
    "    if ssmiRttovM != None:   # if an RTTOV instance for multiple profiles has been defined\n",
    "        \n",
    "        # Initialize multiple profiles for using a single call to RTTOV\n",
    "        # This is to be passed to the Jacobian function inside pyOpEst:\n",
    "        # The Jacobian is needed per parameter (len(xa.index)+len(xb.index))\n",
    "\n",
    "        nProfilesM = len(xa.index)+len(xb.index) # total number of parameters (x and b)\n",
    "            \n",
    "        # Load Atlases for Multiple profiles: (if any)    \n",
    "        #ssmiRttovM.SurfEmisRefl = np.zeros((\n",
    "        #    4, nProfilesM, len(channels_list)), dtype=np.float64) # RTTOVv12 used (2,nprof,nchan)\n",
    "                   \n",
    "        myProfilesM = pyrttov.Profiles(nProfilesM, nlev) # \n",
    "\n",
    "        #press2 = np.ones((nlev, nProfilesM))*Pressure # Pressure:(nlev,1)\n",
    "      \n",
    "        # Initialize profile datastructure for use in Jacobian computation:        \n",
    "        forward_b_init(np.ones((nlev, nProfilesM))*Pressure, salinity,\n",
    "                       lat, lon, datetime_obs, zenithAngle, myProfilesM) \n",
    "    \n",
    "        forwardKwArgsM = {\"myProfiles_a\" : myProfilesM, \n",
    "        \"ssmiRttov_a\" : ssmiRttovM,\n",
    "        \"channels_list\":channels_list.tolist()}\n",
    "    else:\n",
    "        forwardKwArgsM = None    # if not, then use only the single profile RTTOV instance;\n",
    "                                 # this results in a much slower Jacobian calculation.\n",
    "        \n",
    "        \n",
    "    # Create OE object:\n",
    "    \n",
    "    oe_ref = pyOE.optimalEstimation( # oe_1 if windDisambiguation used\n",
    "        x_vars, # state variable names\n",
    "        xa,  # a priori\n",
    "        Sa, # a priori uncertainty\n",
    "        channels_list,  # measurement variable names\n",
    "        y_obs, # observations\n",
    "        S_y, # observation uncertainty\n",
    "        forwardRT, # forward Operator\n",
    "        userJacobian=rttovK, # RTTOV's K model operator\n",
    "        forwardKwArgs=forwardKwArgs, # additonal function arguments\n",
    "        #multipleForwardKwArgs=forwardKwArgsM, # additonal function arguments for jacobian \n",
    "        #x_truth=x_truth, # true profile\n",
    "        b_vars=b_vars,   # Parameter vector variable names\n",
    "        b_p=xb,        # Parameter vector \n",
    "        S_b=Sb,        # Parameters error covariance matrix \n",
    "        perturbation=0.01,\n",
    "        verbose=False\n",
    "    )\n",
    "    return oe_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TB dataset:\n",
    "\n",
    "times = xr.open_dataset(BT_dir2+BT_file2+'5chan'+'.nc').time\n",
    "print(type(times))\n",
    "# nearest to user input in dataset:\n",
    "init_date = times.sel(time=init_date, method = \"nearest\")\n",
    "end_date = times.sel(time=end_date, method = \"nearest\")\n",
    "\n",
    "times = None\n",
    "\n",
    "DS_CMSAF_ocean = xr.open_dataset(BT_dir2+BT_file2+'5chan'+'.nc'\n",
    "                                    ).sel(time=slice(init_date,end_date),\n",
    "                                          #scene_channel=np.array([11,12,14,15]),\n",
    "                                         ).chunk({\"time\": chunk_size_time,\n",
    "                                             \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "#DS_CMSAF_ocean_all = xr.open_dataset(BT_dir2+BT_file2+'5chan'+'.nc'\n",
    "#                                    ).chunk({\"time\": chunk_size_time,\n",
    "#                                             \"scene_across_track\": chunk_size_s_a_t})\n",
    "# nearest to user input in dataset:\n",
    "#init_date = DS_CMSAF_ocean_all.time.sel(time=init_date, method = \"nearest\")\n",
    "#end_date = DS_CMSAF_ocean_all.time.sel(time=end_date, method = \"nearest\")\n",
    "\n",
    "# Dataset over the ocean ready to be analyzed:\n",
    "#DS_CMSAF_ocean = DS_CMSAF_ocean_all.sel(time=slice(init_date,end_date)\n",
    "#                                       ).chunk({\"time\": chunk_size_time,\"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "DS_CMSAF_ocean['wind'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Near surface wind speed (NSWP)',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "DS_CMSAF_ocean['wind_err'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'NSWP uncertainty',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "if(performTests):\n",
    "    sizeResults = 6 # parameter on apply_ufunc\n",
    "    workerMemLimit = '4GB' # For cluster definition (works well for 1 hour of CMSAF data)\n",
    "    DS_CMSAF_ocean['chiSquareTest1'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with observation in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "    DS_CMSAF_ocean['chiSquareTest2'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Observation agrees with prior in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "    DS_CMSAF_ocean['chiSquareTest3'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with prior in Y space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "    DS_CMSAF_ocean['chiSquareTest4'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with priot in X space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "    \n",
    "else:\n",
    "    sizeResults = 2 # parameter on apply_ufunc\n",
    "    workerMemLimit = '2GB' # For cluster definition (works well for 1 hour of CMSAF data)\n",
    "    \n",
    "DS_CMSAF_ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS_CMSAF_ocean.global_channel_ID.compute()\n",
    "#dask.visualize(DS_CMSAF_ocean.channel_uncertainty, filename='ch_ID_NO_chunks.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.visualize(DS_CMSAF_ocean.tb, filename='tb_tasks_chunks.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def testFunc(tb):\n",
    "#    tb2 = tb[0]\n",
    "#    return tb2\n",
    "\n",
    "def retrieveWind(time, tb, global_channel,\n",
    "                 channel_uncertainty,\n",
    "                 lat, lon, eia, #dir_bands2,\n",
    "                 #lat_bands2, \n",
    "                 performTests,\n",
    "                ):\n",
    "    \n",
    "    wind = np.full(lat.shape, np.nan, dtype=np.float32)\n",
    "    wind_err = np.full(lat.shape, np.nan, dtype=np.float32)\n",
    "    if(performTests):\n",
    "        chiSquareTest1 = np.full(lat.shape,np.nan,dtype=np.float32)\n",
    "        chiSquareTest2 = np.full(lat.shape,np.nan,dtype=np.float32)\n",
    "        chiSquareTest3 = np.full(lat.shape,np.nan,dtype=np.float32)\n",
    "        chiSquareTest4 = np.full(lat.shape,np.nan,dtype=np.float32)\n",
    "\n",
    "    if((~np.isnan(lat))and(~np.any(np.isnan(tb)))):  # Input dataset has been filtered for sea-land flag (so both, lat and lon are valid or nan)\n",
    "        \n",
    "        oe_ref = init_retrieval_data(time,\n",
    "                                     lat, lon,\n",
    "                                     global_channel,\n",
    "                                     channel_uncertainty,\n",
    "                                     tb,\n",
    "                                     eia,\n",
    "                                     #dir_bands2,\n",
    "                                     #lat_bands2\n",
    "                                    )\n",
    "\n",
    "        oe_ref.doRetrieval()     \n",
    "    \n",
    "        if oe_ref.converged:  # \n",
    "            _, _, w10m, _, _,_ = supporting_routines_m.splitX_all_2W(oe_ref.x_op)\n",
    "            _, _, w10m_err, _, _,_ = supporting_routines_m.splitX_all_2W(oe_ref.x_op_err)\n",
    "    \n",
    "            wind =  w10m.values.item()\n",
    "            wind_err =  w10m_err.values.item()\n",
    "            \n",
    "            if(performTests):      \n",
    "                #aux_test = oe_ref.chiSquareTest()\n",
    "                chiSquareTest1 = np.float32(oe_ref.chiSquareTest()[0][0])\n",
    "                chiSquareTest2 = np.float32(oe_ref.chiSquareTest()[0][1])\n",
    "                chiSquareTest3 = np.float32(oe_ref.chiSquareTest()[0][2])\n",
    "                chiSquareTest4 = np.float32(oe_ref.chiSquareTest()[0][3])\n",
    "                #print(aux_test[0][0])\n",
    "                #print(aux_test[0][1])\n",
    "                #chiSquareTest1 = np.float32(aux_test[0][0])\n",
    "                #chiSquareTest2 = np.float32(aux_test[0][1])\n",
    "                #chiSquareTest3 = np.float32(aux_test[0][2])\n",
    "                #chiSquareTest4 = np.float32(aux_test[0][3])\n",
    "    \n",
    "    if(performTests):\n",
    "        out = np.array([wind, wind_err,\n",
    "                    chiSquareTest1,\n",
    "                    chiSquareTest2,\n",
    "                    chiSquareTest3,\n",
    "                    chiSquareTest4,])\n",
    "    else:\n",
    "        out = np.array([wind, wind_err,])\n",
    "        \n",
    "    return out  \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis (retrievals)\n",
    "\n",
    "#os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\" #\"65536\" #\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "startTimeAll = time.time()\n",
    "\n",
    "# Group per hour or couple of hours in order to avoid having memory issues in workers:\n",
    "\n",
    "DS_CMSAF_ocean_grouped = DS_CMSAF_ocean.resample(time='1H') #.groupby(\"time.hour\")\n",
    "i=0\n",
    "\n",
    "client = Client(\"tcp://10.120.79.130:8786\")\n",
    "\n",
    "for hour_name, hour_group in DS_CMSAF_ocean_grouped:\n",
    "    \n",
    "    startTimeHour = time.time()\n",
    "    \n",
    "    #hour_group = hour_group.chunk({\"time\": chunk_size_time,\n",
    "    #                               \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "    #with LocalCluster(threads_per_worker = 1, \n",
    "    #                  n_workers = 8, memory_limit = workerMemLimit,\n",
    "    #                  processes=True\n",
    "    #                 ) as cluster, Client(cluster) as client:\n",
    "    #    \n",
    "    out_wind = xr.apply_ufunc(retrieveWind,\n",
    "                                  hour_group.time,\n",
    "                                  hour_group.tb,\n",
    "                                  hour_group.global_channel_ID,\n",
    "                                  hour_group.channel_uncertainty,\n",
    "                                  hour_group.lat, hour_group.lon,\n",
    "                                  hour_group.eia,# dir_bands2,\n",
    "                                  #lat_bands2,\n",
    "                                  performTests,\n",
    "                                  input_core_dims=[[], \n",
    "                                               [\"scene_channel\"], \n",
    "                                               [\"scene_channel\"],\n",
    "                                               [\"scene_channel\"],[],[],[],[],#[],[],\n",
    "                                              ],\n",
    "                                  exclude_dims= set((\"scene_channel\",)),\n",
    "                                  output_core_dims=[\n",
    "                                      [\"results\"],\n",
    "                                  ],\n",
    "                                  dask=\"parallelized\",\n",
    "                                  output_dtypes=[hour_group.wind.dtype],\n",
    "                                  #output_sizes={\"results\": sizeResults}, # 6 if performTests=True, 2 otherwise\n",
    "                                  dask_gufunc_kwargs={\n",
    "                                      'output_sizes':{\n",
    "                                          \"results\": sizeResults}},\n",
    "                                  vectorize=True\n",
    "                                 ).compute().chunk({\"time\": chunk_size_time,\n",
    "                                                    \"scene_across_track\": chunk_size_s_a_t})\n",
    "        \n",
    "        #dask.visualize(out_wind, filename='retrieval_tasks.png')\n",
    "        #out_wind.compute().chunk({\"time\": chunk_size_time,\n",
    "        #                          \"scene_across_track\": chunk_size_s_a_t})   \n",
    "    print(\"%.2f s , Time_Hour\" % (time.time()-startTimeHour)) \n",
    "\n",
    "    if(i==0):\n",
    "        out_wind_1 = out_wind    \n",
    "    else:\n",
    "        out_wind_1 = xr.concat((out_wind_1, out_wind), dim = \"time\")\n",
    "\n",
    "    print(\"Time group:\")\n",
    "    print(i)\n",
    "    print(\"Done!\")\n",
    "    i+=1\n",
    "    client.restart()\n",
    "\n",
    "# Done with processing, close this client:\n",
    "client.close()\n",
    "    \n",
    "DS_CMSAF_ocean[\"wind\"].data = out_wind_1.data[:,:,0]\n",
    "DS_CMSAF_ocean[\"wind_err\"].data = out_wind_1.data[:,:,1]\n",
    "if(performTests):\n",
    "    DS_CMSAF_ocean[\"chiSquareTest1\"].data = out_wind_1.data[:,:,2]\n",
    "    DS_CMSAF_ocean[\"chiSquareTest2\"].data = out_wind_1.data[:,:,3]\n",
    "    DS_CMSAF_ocean[\"chiSquareTest3\"].data = out_wind_1.data[:,:,4]\n",
    "    DS_CMSAF_ocean[\"chiSquareTest4\"].data = out_wind_1.data[:,:,5]\n",
    "    \n",
    "\n",
    "print(\"%.2f s , TimeAll_hour\" % (time.time()-startTimeAll))\n",
    "\n",
    "# Write ds including retrieved winds and wind uncertainty:\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with LocalCluster(threads_per_worker = 1, \n",
    "                  n_workers = 8, memory_limit = workerMemLimit,\n",
    "                  processes=True\n",
    "                 ) as cluster, Client(cluster) as client:\n",
    "    delayed_obj = DS_CMSAF_ocean.to_netcdf(BT_dir2+BT_file2+\"Winds\"+'.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.visualize(out_wind, filename='retrieval_tasks.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS_CMSAF_ocean2 = xr.open_dataset(BT_dir2+BT_file2+\"Winds\"+'.nc'\n",
    "#                                    ) #.chunk({\"time\": chunk_size_time,\n",
    "#                                      #       \"scene_across_track\": chunk_size_s_a_t})\n",
    "#DS_CMSAF_ocean2     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapPlotScat(x,y,data,namefile, mini, maxi, orthoCenter=None):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    #ortho = ccrs.Orthographic(0,-15) # ccrs.Orthographic(60,-15)\n",
    "    ortho = ccrs.PlateCarree()\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    #crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.PlateCarree() #ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black',linewidth=0.1)\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.07, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER  \n",
    "    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.01, #0.15\n",
    "        marker = \"o\",\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        vmin=mini, vmax=maxi,  # 3,18\n",
    "        #vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"10m Wind Speed, RadEst [m/s]\")\n",
    "    #fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=450)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()\n",
    "\n",
    "swath_radEst = SwathDefinition(lons=DS_CMSAF_ocean.lon.values, lats=DS_CMSAF_ocean.lat.values)\n",
    "lons_radEst, lats_radEst = swath_radEst.get_lonlats()\n",
    "\n",
    "world_lons, world_lats, world_wind_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.wind.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_err_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.wind_err.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_chis1_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.chiSquareTest1.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_chis2_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.chiSquareTest2.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_chis3_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.chiSquareTest3.values,\n",
    "                            radius_of_influence=1000)\n",
    "world_lons, world_lats, world_wind_chis4_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, DS_CMSAF_ocean.chiSquareTest4.values,\n",
    "                            radius_of_influence=1000)\n",
    "\n",
    "# Some diagnostics\n",
    "DS_CMSAF_ocean['chisALL'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with priot in X space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "DS_CMSAF_ocean['chisALL'].data = np.float32(((DS_CMSAF_ocean.chiSquareTest1==True\n",
    "                                            )&(DS_CMSAF_ocean.chiSquareTest2==True\n",
    "                                                      )&(DS_CMSAF_ocean.chiSquareTest3==True\n",
    "                                                                )&(DS_CMSAF_ocean.chiSquareTest4==True)))\n",
    "\n",
    "world_lons, world_lats, world_wind_chisALL_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, (DS_CMSAF_ocean.chisALL.values),\n",
    "                            radius_of_influence=1000)\n",
    "\n",
    "DS_CMSAF_ocean['chisT0andT1'] = xr.DataArray(\n",
    "                data   = np.full((DS_CMSAF_ocean.time.shape[0],\n",
    "                                  DS_CMSAF_ocean.scene_across_track.shape[0]),\n",
    "                                 np.nan, dtype = np.float32),   # enter data here\n",
    "                dims   = ['time','scene_across_track'],\n",
    "                coords = {'time': DS_CMSAF_ocean.time, \n",
    "                          'scene_across_track':DS_CMSAF_ocean.scene_across_track},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'Optimal solution agrees with priot in X space',\n",
    "                    'units'     : 'True/False'\n",
    "                    }\n",
    "                ).chunk({\"time\": chunk_size_time,\n",
    "                         \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "DS_CMSAF_ocean['chisT0andT1'].data = np.float32((DS_CMSAF_ocean.chiSquareTest1==True\n",
    "                                            )&(DS_CMSAF_ocean.chiSquareTest2==False))\n",
    "\n",
    "world_lons, world_lats, world_wind_chisT0andT1_radEst = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_radEst, lats_radEst, (DS_CMSAF_ocean.chisT0andT1.values),\n",
    "                            radius_of_influence=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_radEst,\n",
    "                 'World_wind_RadEst_Plat', 3,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_err_radEst,\n",
    "                 'World_wind_Err_RadEst_Plat',0.5,1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPlotScat(world_lons, world_lats, world_wind_chis1_radEst,\n",
    "                 'World_chisqu1_RadEst_Plat',0,1)\n",
    "mapPlotScat(world_lons, world_lats, world_wind_chis2_radEst,\n",
    "                 'World_chisqu2_RadEst_Plat',0,1)\n",
    "mapPlotScat(world_lons, world_lats, world_wind_chis3_radEst,\n",
    "                 'World_chisqu3_RadEst_Plat',0,1)\n",
    "mapPlotScat(world_lons, world_lats, world_wind_chis4_radEst,\n",
    "                 'World_chisqu4_RadEst_Plat',0,1)\n",
    "\n",
    "mapPlotScat(world_lons, world_lats, world_wind_chisALL_radEst,\n",
    "                 'world_wind_chisALL_radEst',0,1)\n",
    "mapPlotScat(world_lons, world_lats, world_wind_chisT0andT1_radEst,\n",
    "                 'world_wind_chisT0andT1_radEst',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_TB_frame(BT_scene_env1, area_interest, begin_t, end_t):\n",
    "    \n",
    "    lat_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lat[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    lon_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.lon[begin_t:end_t,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "    tb_scene1 = supporting_routines_m.generate_masked_array(BT_scene_env1.tb[begin_t:end_t,:,:],\n",
    "                                                  BT_scene_env1.sft[begin_t:end_t,:], 0, '==', drop= False) \n",
    "\n",
    "    grid_lons_interest, grid_lats_interest = area_interest.get_lonlats()\n",
    "\n",
    "    swath_scene1 = SwathDefinition(lons=lon_scene1, lats=lat_scene1)\n",
    "    lons_scene1, lats_scene1 = swath_scene1.get_lonlats()\n",
    "\n",
    "    reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "\n",
    "    return reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1\n",
    "    #swath_reduced_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def defineArea(corners, proj_id, datum):\n",
    "    #corners=parseMeta(data_name)\n",
    "\n",
    "    lat_0 = '{lat_0:5.2f}'.format_map(corners)\n",
    "    lon_0= '{lon_0:5.2f}'.format_map(corners)\n",
    "    lon_bbox = [corners['min_lon'],corners['max_lon']]\n",
    "    lat_bbox = [corners['min_lat'],corners['max_lat']]\n",
    "#    area_dict = dict(datum=datum,lat_0=lat_0,lon_0=lon_0,\n",
    "#                proj=proj_id,units='m')\n",
    "\n",
    "    area_dict = dict(datum=datum,lat_0=-15,lon_0=60,\n",
    "                proj=proj_id,units='m',a=6370997.0,)\n",
    "\n",
    "    prj=pyproj.Proj(area_dict)\n",
    "    x, y = prj(lon_bbox, lat_bbox)\n",
    "    xsize=200\n",
    "    ysize=200\n",
    "    area_id = 'granule'\n",
    "    area_name = 'modis swath 5min granule'\n",
    "    area_extent = (x[0], y[0], x[1], y[1])\n",
    "    print(area_extent)\n",
    "    area_def = AreaDefinition(area_id, area_name, proj_id, \n",
    "                                   area_dict, xsize, ysize,area_extent)\n",
    "    return area_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation of area of interest:\n",
    "#corners = {\"min_lon\": 25 , \"max_lon\": 75, \"min_lat\": -30 , \"max_lat\": 0, \"lat_0\": 60, \"lon_0\":-15}\n",
    "corners = {\"min_lon\": -95 , \"max_lon\": 20, \"min_lat\": 3 , \"max_lat\": 50, \"lat_0\": 27, \"lon_0\":-57}\n",
    "proj_id = 'eqc'  # eqc\n",
    "datum = 'WGS84'\n",
    "area_interest = defineArea(corners, proj_id, datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "area_def_world = load_area('areas.yaml', 'worldeqc30km70')# 'worldeqc30km70') # for plots\n",
    "grid_lons_world, grid_lats_world = area_def_world.get_lonlats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeMapAnimScat(BT_scene, BT_attributes, channel, area, \n",
    "                      init_date, nFrames, delta_hours, namefile):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(frameon=False) #figsize=(8, 6))\n",
    "    fig.add_axes([0,0,1,1])\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    ax.set_title(\"TB \"+namefile)\n",
    "    #ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black') \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "\n",
    "\n",
    "    #delta_hours = 12\n",
    "    end_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "    time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "        BT_attributes.time.values<end_date))\n",
    "    begin_t = time_slice[0][0]  \n",
    "    end_t = time_slice[0][-1]\n",
    "\n",
    "    x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "    \n",
    "    ims = []\n",
    "    im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12     # 180, 270\n",
    "            vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    \n",
    "    #fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    \n",
    "    for i in np.arange(nFrames):\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=data[:,channel],\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            #vmin=3, vmax=12  # 180, 270\n",
    "            vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "        \n",
    "        ims.append([im1])\n",
    "        init_date = init_date + np.timedelta64(delta_hours, 'h') \n",
    "        end_date = end_date + np.timedelta64(delta_hours, 'h') \n",
    "        time_slice = np.where((BT_attributes.time.values>=init_date)&(\n",
    "            BT_attributes.time.values<end_date))\n",
    "        begin_t = time_slice[0][0]  \n",
    "        end_t = time_slice[0][-1] \n",
    "        x, y, data = get_TB_frame(BT_scene, area, begin_t, end_t)\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    plt.tight_layout()\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 12\n",
    "delta_hours = 12\n",
    "channel = 2\n",
    "namefile = 'env1_22V_12h_F16'\n",
    "timeMapAnimScat(BT_scene, BT_attributes, channel, area_interest, \n",
    "                      init_date, nFrames, delta_hours, namefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath scene 1 at a world wide scale:\n",
    "\n",
    "result_scene1 = resample_nearest(swath_scene1, tb_scene1.values, area_def_world, \n",
    "                          radius_of_influence=30000, fill_value=np.nan)\n",
    "\n",
    "#result_scene2 = resample_nearest(swath_scene2, tb_scene2.values, area_def_world, \n",
    "#                          radius_of_influence=30000, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "\n",
    "\n",
    "#reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2 = \\\n",
    "#                           data_reduce.swath_from_lonlat_grid(grid_lons_interest, grid_lats_interest,\n",
    "#                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "#                            radius_of_influence=3000)\n",
    "#swath_reduced_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out swath part that intersects the area of interest\n",
    "world_lons_scene1, world_lats_scene1, world_data_scene1 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene1, lats_scene1, tb_scene1.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene1 = SwathDefinition(reduced_lons_scene1, reduced_lats_scene1)\n",
    "\n",
    "world_lons_scene2, world_lats_scene2, world_data_scene2 = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(grid_lons_world, grid_lats_world,\n",
    "                            lons_scene2, lats_scene2, tb_scene2.values,\n",
    "                            radius_of_influence=3000)\n",
    "swath_world_scene2 = SwathDefinition(reduced_lons_scene2, reduced_lats_scene2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resample swath data into a grid in the area of interest \n",
    "result_reduced_scene1 = resample_nearest(swath_reduced_scene1, reduced_data_scene1, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)\n",
    "\n",
    "result_reduced_scene2 = resample_nearest(swath_reduced_scene2, reduced_data_scene2, area_interest, \n",
    "                                  radius_of_influence=30000, fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapArea('mapArea0', area_def_world)\n",
    "mapArea('mapArea01', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chann = 0\n",
    "# Plot resampled (grid version) scenes:\n",
    "basicMapPlot(result_scene1[:,:,chann],'scene1'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene1[:,:,chann],\n",
    "             'scene1_reduced'+str(chann), area_interest)  # map only the area of interest, grid\n",
    "\n",
    "basicMapPlot(result_scene2[:,:,chann],'scene2'+str(chann), area_def_world)  # map of the whole world, grid\n",
    "basicMapPlot(result_reduced_scene2[:,:,chann],\n",
    "             'scene2_reduced'+str(chann), area_interest)  # map only the area of interest, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in area of interest, Plate Carree projection\n",
    "\n",
    "chann = 0\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19H', area_interest, )\n",
    "chann = 1\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_19V', area_interest, )\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'scene1_scatt_PCarr_22V', area_interest, )\n",
    "#chann = 1\n",
    "#basicMapPlotScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene2_scatt_PCarr_91V', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path('/nobackup/users/echeverr/fortran_tests/netcdf/following_Edouard/imgs/ch19H')\n",
    "images = list(image_path.glob('*.png'))\n",
    "image_list = []\n",
    "for file_name in images:\n",
    "    image_list.append(imageio.imread(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimwrite('animated_from_images.gif', image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot in world area, Orthographic projection\n",
    "\n",
    "chann = 2\n",
    "# Plot original swath pixels:\n",
    "basicMapPlotScat1(reduced_lons_scene1, reduced_lats_scene1, reduced_data_scene1[:,chann],\n",
    "                 'world_scene1_scatt_Orth_22V', area_interest)\n",
    "#chann = 3\n",
    "#basicMapPlotScat1(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'world_scene1_scatt_Orth_91H', area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot original swath pixels:\n",
    "chann = 2\n",
    "basicMapPlotScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'scene1_scatt_Orth_world_22V', area_def_world)\n",
    "chann = 3\n",
    "basicMapPlotScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'scene1_scatt_Orth_world_91H', area_def_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlot(result,namefile, area):\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    #ax.background_img(name='BM', resolution='high') \n",
    "    ax.coastlines();\n",
    "    #ax.stock_img();\n",
    "    ax.grid(True)\n",
    "    #ax.set_xlabel('Longitude [deg]')\n",
    "    #ax.set_ylabel('Latitude [deg]')\n",
    "\n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    #gl.xlabels_top = False\n",
    "    #gl.ylabels_left = False\n",
    "    #gl.ylabels_right=True\n",
    "    #gl.xlines = True\n",
    "    #gl.xlocator = mticker.FixedLocator([70, 75, 80, 85])\n",
    "    #gl.ylocator = mticker.FixedLocator([-5, -3, -1, 1, 3])\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    im = ax.imshow(result, transform=crs, extent=crs.bounds, origin='upper', cmap='jet', vmin=150, vmax=250)\n",
    "    fig.colorbar(im,ax=ax) \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)   \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "    ax.set_title(\"TB\")\n",
    "    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "                      color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER    \n",
    "\n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        #vmin=3, vmax=18         #180, 270\n",
    "        vmin=130, vmax=270         #180, 270\n",
    "    )\n",
    "    fig.colorbar(im).set_label(\"Brightness temperature [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapArea(namefile, area):\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    fig, ax = plt.subplots(figsize=(13, 3))\n",
    "    ax= plt.axes(projection=crs)\n",
    "    \n",
    "    #gl = ax.gridlines(crs=ccrs.Orthographic(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl.yformatter = LATITUDE_FORMATTER\n",
    "    \n",
    "    ax.coastlines(linewidth=0.5)   \n",
    "    #ax.set_global() \n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chann = 0\n",
    "#basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "#                 'scene1_scatt'+str(chann), area_interest)\n",
    "\n",
    "nFrames = 150\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat(reduced_lons_scene2, reduced_lats_scene2, reduced_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_red_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    crs = area.to_cartopy_crs()\n",
    "    \n",
    "    #fig2 = plt.subplots(1,2) \n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.axes(projection=crs)  \n",
    "    ax.set_global()\n",
    "    gl = ax.gridlines()  \n",
    "    #ax.set_title(\"TB \"+namefile)\n",
    "    ax.set_title(\"Wind Speed \"+namefile)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "   \n",
    "    #gl0 = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.1, \n",
    "    #                  color='black', alpha=0.5, linestyle='--', draw_labels=True)\n",
    "\n",
    "    #gl0.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl0.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = x[start_:end_]\n",
    "    y2 = y[start_:end_]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12     # 180, 270\n",
    "            #vmin=130, vmax=270     # 180, 270\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=12  # 180, 270\n",
    "            #vmin=130, vmax=270  # 180, 270\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,x[start_:end_])\n",
    "        y2 = np.append(y2,y[start_:end_])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'_bar.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BT_scene_env2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapPlotScat1(x,y,data,namefile, area):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ortho = ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    #crs = ccrs.Orthographic(60,-15)\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()    \n",
    "    \n",
    "    #ax.set_title(\"TB\")\n",
    "    #ax.coastlines() \n",
    "    # Plot the air temperature as colored circles and the wind speed as vectors.\n",
    "    im = ax.scatter(\n",
    "        xy[:,0],\n",
    "        xy[:,1],\n",
    "        c=data,\n",
    "        s=0.15,\n",
    "        cmap=\"viridis\",\n",
    "        #transform=crs,\n",
    "        #vmin=3, vmax=18,  # 180, 270\n",
    "        vmin=130, vmax=270,  # 180, 270        \n",
    "    )\n",
    "    #fig.colorbar(im).set_label(\"10m Wind Speed, HOAPS [m/s]\")\n",
    "    fig.colorbar(im).set_label(\"Temp. Bright [K]\")\n",
    "    \n",
    "# Use an utility function to add tick labels and land and ocean features to the map.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(namefile+'.png', bbox_inches='tight', dpi=300)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = 1200\n",
    "start_frame = 0\n",
    "nAcrossSwathFrame = 180\n",
    "\n",
    "chann = 0\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_19H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 2\n",
    "basicMapAnimScat1(world_lons_scene1, world_lats_scene1, world_data_scene1[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_22V', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)\n",
    "\n",
    "chann = 3\n",
    "basicMapAnimScat1(world_lons_scene2, world_lats_scene2, world_data_scene2[:,chann],\n",
    "                 'An_scene1_scatt_Orth_world_91H', area_interest, \n",
    "                  nFrames, start_frame, nAcrossSwathFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basicMapAnimScat1(x,y,data,namefile, area, \n",
    "                      nFrames, start_frame, nAcrossSwathFrame):\n",
    "    # Make a Mercator map of the data using Cartopy\n",
    "    \n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    ortho = ccrs.Orthographic(-39,18) #ccrs.Orthographic(60,-15)\n",
    "    ax = plt.axes(projection=ortho)\n",
    "    \n",
    "    crs = ccrs.RotatedPole(pole_longitude=177.5, pole_latitude=37.5)\n",
    "    geo = ccrs.Geodetic()\n",
    "    \n",
    "    ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "    \n",
    "    xy = ortho.transform_points(geo, x, y)\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.gridlines()        \n",
    "       \n",
    "    ax.set_title(\"Wind Speed \"+namefile) \n",
    "    #ax.set_title(\"Temperature Brightness \"+namefile) \n",
    "    \n",
    "    start_ = start_frame\n",
    "    end_ = nAcrossSwathFrame\n",
    "    \n",
    "    ims = []\n",
    "    x2 = xy[start_:end_,0]\n",
    "    y2 = xy[start_:end_,1]\n",
    "    data2_0 = data[start_:end_]\n",
    "    #data2_2 = data[start_:end_,2]\n",
    "\n",
    "    im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18  # 180, 250\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "    fig.colorbar(im1).set_label(\"10m Wind Speed [m/s]\")\n",
    "    #fig.colorbar(im1).set_label(\"Temp. Bright. [K]\") \n",
    "    for i in np.arange(nFrames):\n",
    "        #ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        im1 = plt.scatter(\n",
    "            x2,\n",
    "            y2,\n",
    "            c=data2_0,\n",
    "            s=0.15,\n",
    "            cmap=\"viridis\",\n",
    "            #transform=ccrs.PlateCarree(),\n",
    "            vmin=3, vmax=18   # 180, 270 TB\n",
    "            #vmin=130, vmax=270   # 180, 270 TB\n",
    "        ) \n",
    "\n",
    "        ims.append([im1])\n",
    "      \n",
    "        \n",
    "        start_ +=nAcrossSwathFrame\n",
    "        end_ +=nAcrossSwathFrame\n",
    "        x2 = np.append(x2,xy[start_:end_,0])\n",
    "        y2 = np.append(y2,xy[start_:end_,1])\n",
    "        data2_0 = np.append(data2_0,data[start_:end_])\n",
    "        #data2_2 = np.append(data2_2,data[start_:end_,2])\n",
    "\n",
    "        \n",
    "    im_ani = animation.ArtistAnimation(fig, ims, interval=30, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "    # To save this second animation with some metadata, use the following command:\n",
    "    # im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "    im_ani.save(namefile+'.mp4',dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
